{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "outputId": "b77ead25-58d8-49e1-896f-f72efd8fb5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "import sys\n",
        "import time\n",
        "import unittest\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.constraints import non_neg\n",
        "from scipy.misc import imread\n",
        "\n",
        "!pip install scikit-optimize\n",
        "import skopt\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.20.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    a, b = np.ravel(a), np.ravel(b)\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / max(np.linalg.norm(a) * np.linalg.norm(b), sys.float_info.epsilon)\n",
        "\n",
        "\n",
        "def keras_norm(vect):\n",
        "    return K.sqrt(K.batch_dot(vect, vect, axes=1))\n",
        "\n",
        "\n",
        "def keras_similarity(vects):\n",
        "    x, y = vects\n",
        "    # Cosine similarity\n",
        "    return K.batch_dot(x, y, axes=1) / K.maximum(keras_norm(x) * keras_norm(y), K.epsilon())\n",
        "\n",
        "\n",
        "def sim_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, output_size=128, map_reg_rate=0, scale_reg_rate=0):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = input\n",
        "    x = Dense(output_size, kernel_regularizer=regularizers.l2(map_reg_rate))(x)\n",
        "    x = Dense(output_size, kernel_initializer='identity', kernel_constraint=non_neg(),\n",
        "              kernel_regularizer=regularizers.l2(scale_reg_rate), use_bias=False)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def syn_accuracy(y_true, y_pred):\n",
        "    '''Compute synonym classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = y_pred.ravel() > median\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def ant_accuracy(y_true, y_pred):\n",
        "    '''Compute antonym classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = -((y_pred.ravel() < median).astype(np.int, casting='safe', copy=False))\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def keras_syn_accuracy(y_true, y_pred):\n",
        "    '''Compute synonym classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.75, y_true.dtype)))\n",
        "\n",
        "\n",
        "def keras_ant_accuracy(y_true, y_pred):\n",
        "    '''Compute antonym classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < -0.75, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxHK8Dd1z4k1",
        "colab_type": "code",
        "outputId": "8213d920-0ce1-46b0-f155-11c38391a6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip -u glove.6B.zip || (wget http://nlp.stanford.edu/data/glove.6B.zip && unzip -u glove.6B.zip)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n",
            "--2019-01-25 22:36:07--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-01-25 22:36:07--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.10MB/s    in 88s     \n",
            "\n",
            "2019-01-25 22:37:36 (9.30 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFVWMt_f0fWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 50K words is enough for 95% of English word usage, per the OED:\n",
        "# https://web.archive.org/web/20160304170936/http://www.oxforddictionaries.com/words/the-oec-facts-about-the-language\n",
        "# The GloVe text files appear to be roughly sorted by frequency of usage.\n",
        "!egrep '^[a-z]+ ' glove.6B.300d.txt > glove.head.txt\n",
        "!cut -d' ' -f1 glove.head.txt > glove.head.strings.txt\n",
        "!cut -d' ' -f2- glove.head.txt > glove.head.vectors.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9XC_PYBZaZFy",
        "colab_type": "code",
        "outputId": "3a5cab98-dde3-486d-eb06-23a30cf04730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!wc glove.6B.300d.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    400000  120400000 1037962819 glove.6B.300d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q2Q9l3PN110t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_strings = np.loadtxt('glove.head.strings.txt', dtype=object)\n",
        "word_vectors = np.loadtxt('glove.head.vectors.txt')\n",
        "input_shape = word_vectors.shape[1:]\n",
        "assert len(word_strings) == len(word_vectors)\n",
        "\n",
        "index_for_word = {}\n",
        "for i, word in enumerate(word_strings):\n",
        "    index_for_word[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T84gxLzJU9B",
        "colab_type": "code",
        "outputId": "a6bc3261-3d3c-4a8e-f5d3-29266ba9385f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def might_be_synonyms(w1, w2):\n",
        "    s1 = set()\n",
        "    d1 = set()\n",
        "    s2 = set()\n",
        "    d2 = set()\n",
        "    for synset in wn.synsets(w1):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        s1.update(lemma_names)\n",
        "        d1.update(nltk.word_tokenize(synset.definition()))\n",
        "    for synset in wn.synsets(w2):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        s2.update(lemma_names)\n",
        "        d2.update(nltk.word_tokenize(synset.definition()))\n",
        "    total_intersection = len(s1.intersection(s2)) + len(d1.intersection(s2)) + len(d2.intersection(s1))\n",
        "    return total_intersection > 0\n",
        "\n",
        "\n",
        "class TestSynonyms(unittest.TestCase):\n",
        "    def test_true(self):\n",
        "        pairs = [\n",
        "            ('car', 'auto'),\n",
        "            ('auto', 'car'),\n",
        "            ('car', 'railcar'),\n",
        "            ('small', 'tiny'),\n",
        "            ('small', 'miniature'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertTrue(might_be_synonyms(word1, word2))\n",
        "    def test_false(self):\n",
        "        pairs = [\n",
        "            ('car', 'airplane'),\n",
        "            ('car', 'fast'),\n",
        "            ('small', 'focused'),\n",
        "            ('small', 'accidental'),\n",
        "            ('small', 'rabbit'),\n",
        "            ('small', 'flippant'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertFalse(might_be_synonyms(word1, word2))\n",
        "\n",
        "\n",
        "def get_antonyms(word):\n",
        "    result = set()\n",
        "    for lemma in wn.lemmas(word):\n",
        "        for antonym in lemma.antonyms():\n",
        "            result.add(antonym.name())\n",
        "    return result\n",
        "\n",
        "\n",
        "def might_be_antonyms(w1, w2):\n",
        "    w1_antonyms = get_antonyms(w1)\n",
        "    w2_antonyms = get_antonyms(w2)\n",
        "    for w1_antonym in w1_antonyms:\n",
        "        if might_be_synonyms(w2, w1_antonym):\n",
        "            return True\n",
        "    for w2_antonym in w2_antonyms:\n",
        "        if might_be_synonyms(w1, w2_antonym):\n",
        "            return True\n",
        "\n",
        "\n",
        "class TestAntonyms(unittest.TestCase):\n",
        "    def test_get_antonyms(self):\n",
        "        self.assertEqual(get_antonyms('big'), set(['little', 'small']))\n",
        "        self.assertEqual(get_antonyms('fast'), set(['slow']))\n",
        "        self.assertEqual(get_antonyms('big'), set(['little', 'small']))\n",
        "    def test_true(self):\n",
        "        pairs = [\n",
        "            ('big', 'small'),\n",
        "            ('big', 'minor'),\n",
        "            ('big', 'tiny'),\n",
        "            ('big', 'miniature'),\n",
        "            ('fast', 'slow'),\n",
        "            ('fast', 'sluggish'),\n",
        "            ('loud', 'soft'),\n",
        "            ('loud', 'quiet'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertTrue(might_be_antonyms(word1, word2))\n",
        "    def test_false(self):\n",
        "        pairs = [\n",
        "            ('big', 'huge'),\n",
        "            ('big', 'clean'),\n",
        "            ('fast', 'speedy'),\n",
        "            ('fast', 'unusual'),\n",
        "            ('loud', 'noisy'),\n",
        "            ('loud', 'flat'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertFalse(might_be_antonyms(word1, word2))\n",
        "\n",
        "\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
        "\n",
        "\n",
        "def create_pairs(word_strings, word_vectors, class_indices, pos_label, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    string_pairs = []\n",
        "    vector_pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(word_strings))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            np.random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            string_pairs.append([word_strings[anchor], word_strings[pos]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[pos]])\n",
        "            labels.append(pos_label)\n",
        "            \n",
        "            random_neg = None\n",
        "            while random_neg == None or random_neg in family or \\\n",
        "                    might_be_synonyms(word_strings[anchor], word_strings[random_neg]) or \\\n",
        "                    might_be_antonyms(word_strings[anchor], word_strings[random_neg]):\n",
        "                if next_index >= len(shuffled_indices):\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                random_neg = shuffled_indices[next_index]\n",
        "                next_index += 1\n",
        "\n",
        "            string_pairs.append([word_strings[anchor], word_strings[random_neg]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[random_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(string_pairs) == len(vector_pairs) == len(labels)\n",
        "    return np.array(string_pairs), np.array(vector_pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def create_train_dev_test(word_strings, word_vectors, class_count, word_class, pos_label):\n",
        "    classes = list(range(class_count))\n",
        "    random.seed(850101)\n",
        "    random.shuffle(classes)\n",
        "    random.seed()\n",
        "    train_dev_split = round(len(classes)*0.6)\n",
        "    dev_test_split = round(len(classes)*0.8)\n",
        "    train_classes = classes[:train_dev_split]\n",
        "    dev_classes = classes[train_dev_split:dev_test_split]\n",
        "    test_classes = classes[dev_test_split:]\n",
        "    assert len(set(train_classes).intersection(set(dev_classes))) == 0\n",
        "    assert len(set(dev_classes).intersection(set(test_classes))) == 0\n",
        "    assert len(set(train_classes).intersection(set(test_classes))) == 0\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in train_classes]\n",
        "    tr_strings, tr_pairs, tr_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in dev_classes]\n",
        "    dev_strings, dev_pairs, dev_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in test_classes]\n",
        "    te_strings, te_pairs, te_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "    \n",
        "    return tr_strings, tr_pairs, tr_y, dev_strings, dev_pairs, dev_y, te_strings, te_pairs, te_y"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 2.313s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "x6A3h9Wj2Xs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "syn_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_count = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        syn_indices = [index_for_word[syn] for syn in synsets[0].lemma_names() if syn in index_for_word and syn_class[index_for_word[syn]] == -1]\n",
        "        if len(syn_indices) > 1:\n",
        "            syn_class[syn_indices] = class_count\n",
        "            class_count += 1\n",
        "\n",
        "tr_syn_strings, tr_syn_pairs, tr_syn_y, \\\n",
        "dev_syn_strings, dev_syn_pairs, dev_syn_y, \\\n",
        "te_syn_strings, te_syn_pairs, te_syn_y = create_train_dev_test(word_strings, word_vectors, class_count, syn_class, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o13AAwvvaIlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ant_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_count = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    antonyms = get_antonyms(word)\n",
        "    if antonyms:\n",
        "        known_antonyms = [a for a in antonyms if a in index_for_word]\n",
        "        if known_antonyms:\n",
        "            ant_indices = [i, index_for_word[known_antonyms[0]]]\n",
        "            if all(ant_class[ant_indices] == -1):\n",
        "                ant_class[ant_indices] = class_count\n",
        "                class_count += 1\n",
        "\n",
        "tr_ant_strings, tr_ant_pairs, tr_ant_y, \\\n",
        "dev_ant_strings, dev_ant_pairs, dev_ant_y, \\\n",
        "te_ant_strings, te_ant_pairs, te_ant_y = create_train_dev_test(word_strings, word_vectors, class_count, ant_class, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cbbPZrpLSVpd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_model(tr_data, output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff):\n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, output_size)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(keras_similarity,\n",
        "                      output_shape=sim_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "\n",
        "    def contrastive_sim_loss(y_true, y_pred):\n",
        "        return K.mean(-y_true * y_pred +\n",
        "                      K.cast(K.equal(y_true, 0), 'float32') * (K.abs(y_pred) ** deviation_dropoff))\n",
        "\n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_sim_loss, optimizer=opt)\n",
        "    \n",
        "    for tr_pairs, tr_y in tr_data:\n",
        "        model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=0)\n",
        "\n",
        "    return model\n",
        "\n",
        "def eval_model(model):\n",
        "    y_pred = model.predict([tr_syn_pairs[:, 0], tr_syn_pairs[:, 1]])\n",
        "    syn_tr_acc = syn_accuracy(tr_syn_y, y_pred)\n",
        "    y_pred = model.predict([dev_syn_pairs[:, 0], dev_syn_pairs[:, 1]])\n",
        "    syn_dev_acc = syn_accuracy(dev_syn_y, y_pred)\n",
        "\n",
        "    y_pred = model.predict([tr_ant_pairs[:, 0], tr_ant_pairs[:, 1]])\n",
        "    ant_tr_acc = ant_accuracy(tr_ant_y, y_pred)\n",
        "    y_pred = model.predict([dev_ant_pairs[:, 0], dev_ant_pairs[:, 1]])\n",
        "    ant_dev_acc = ant_accuracy(dev_ant_y, y_pred)\n",
        "\n",
        "    print('* Accuracy on synonym training set: %0.2f%%' % (100 * syn_tr_acc))\n",
        "    print('* Accuracy on synonym dev set: %0.2f%%' % (100 * syn_dev_acc))\n",
        "    print('* Accuracy on antonym training set: %0.2f%%' % (100 * ant_tr_acc))\n",
        "    print('* Accuracy on antonym dev set: %0.2f%%' % (100 * ant_dev_acc))\n",
        "    return syn_tr_acc, syn_dev_acc, ant_tr_acc, ant_dev_acc\n",
        "\n",
        "def map_word_vectors(model):\n",
        "    base_network = model.layers[2]\n",
        "    l1_weights = base_network.layers[1].get_weights()[0]\n",
        "    l1_biases = base_network.layers[1].get_weights()[1]\n",
        "    l2_weights = base_network.layers[2].get_weights()[0]\n",
        "    mapped = (word_vectors @ l1_weights + l1_biases) @ l2_weights\n",
        "    mapped_2 = (l2_weights.T @ (l1_weights.T @ word_vectors.T + l1_biases.reshape((-1, 1)))).T\n",
        "    assert np.isclose(mapped, mapped_2).all()\n",
        "    return mapped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPptTE4gShhV",
        "colab_type": "code",
        "outputId": "f8c08cea-4d4c-476b-98a5-88f2c119245e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_syn_pairs])\n",
        "print('* Baseline synonym accuracy on dev set: %0.2f%%' % (100 * syn_accuracy(dev_syn_y, baseline_similarities)))\n",
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_ant_pairs])\n",
        "print('* Baseline antonym accuracy on dev set: %0.2f%%' % (100 * ant_accuracy(dev_ant_y, baseline_similarities)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Baseline synonym accuracy on dev set: 74.65%\n",
            "* Baseline antonym accuracy on dev set: 9.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l562zedZzM_N",
        "colab_type": "code",
        "outputId": "27b4beed-e37c-4da1-a2a7-00150c9b5ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "cell_type": "code",
      "source": [
        "print('Training on synonyms...')\n",
        "model = fit_model([(tr_syn_pairs, tr_syn_y)], 250, 'rms', 256, 30, 0.01, 0.00001, 20)\n",
        "eval_model(model)\n",
        "print('Training on antonyms...')\n",
        "model = fit_model([(tr_ant_pairs, tr_ant_y)], 250, 'rms', 256, 30, 0.01, 0.00001, 20)\n",
        "eval_model(model)\n",
        "print('Training on synonyms & antonyms...')\n",
        "model = fit_model([(np.vstack((tr_syn_pairs, tr_ant_pairs)), np.hstack((tr_syn_y, tr_ant_y)))], 250, 'rms', 256, 30, 0.01, 0.00001, 20)\n",
        "eval_model(model)\n",
        "print('Training on synonyms, then antonyms...')\n",
        "model = fit_model([(tr_syn_pairs, tr_syn_y), (tr_ant_pairs, tr_ant_y)], 250, 'rms', 256, 30, 0.01, 0.00001, 20)\n",
        "eval_model(model)\n",
        "map_word_vectors(model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on synonyms...\n",
            "* Accuracy on synonym training set: 91.94%\n",
            "* Accuracy on synonym dev set: 84.54%\n",
            "* Accuracy on antonym training set: 10.38%\n",
            "* Accuracy on antonym dev set: 9.65%\n",
            "Training on antonyms...\n",
            "* Accuracy on synonym training set: 58.74%\n",
            "* Accuracy on synonym dev set: 58.74%\n",
            "* Accuracy on antonym training set: 74.46%\n",
            "* Accuracy on antonym dev set: 58.42%\n",
            "Training on synonyms & antonyms...\n",
            "* Accuracy on synonym training set: 90.23%\n",
            "* Accuracy on synonym dev set: 82.90%\n",
            "* Accuracy on antonym training set: 25.95%\n",
            "* Accuracy on antonym dev set: 21.78%\n",
            "Training on synonyms, then antonyms...\n",
            "* Accuracy on synonym training set: 59.10%\n",
            "* Accuracy on synonym dev set: 59.17%\n",
            "* Accuracy on antonym training set: 72.32%\n",
            "* Accuracy on antonym dev set: 58.66%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-6.77601418e+00, -4.27214551e+00, -5.53955442e+00, ...,\n",
              "        -5.40324240e+00, -4.75650413e+00, -3.01095209e+00],\n",
              "       [-3.23905499e+00, -2.26440199e+00, -3.70610044e+00, ...,\n",
              "        -3.11671756e+00, -2.11191786e+00, -9.20262692e-01],\n",
              "       [-2.36944889e+00, -1.14235824e+00, -2.40405039e+00, ...,\n",
              "        -2.28590647e+00, -1.82995304e+00, -5.34043462e-01],\n",
              "       ...,\n",
              "       [ 1.69680255e+00,  2.00927996e-03,  2.79827937e+00, ...,\n",
              "         1.74249077e+00,  1.62535258e-01, -1.98569399e+00],\n",
              "       [ 6.18807477e+00,  8.27245955e-01,  3.29448117e+00, ...,\n",
              "         4.67244509e+00,  3.68744692e+00,  2.68172217e-01],\n",
              "       [ 2.88924212e+00, -2.22860132e-01,  1.49813368e+00, ...,\n",
              "         2.35710256e+00,  1.69267762e+00,  5.51473068e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "p1i_j4YWy2La",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        },
        "outputId": "57ee477f-c85b-46f1-be07-efab1ecdfeda"
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Integer(1, 300, name='output_size'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([30], name='epochs'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='map_reg_rate'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='scale_reg_rate'),\n",
        "    skopt.space.Integer(1, 50, name='deviation_dropoff'),\n",
        "]\n",
        "\n",
        "@skopt.utils.use_named_args(space)\n",
        "def objective(output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff):\n",
        "    start_time = datetime.datetime.now()\n",
        "    model = fit_model([(tr_syn_pairs, tr_syn_y)], output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    syn_tr_acc, syn_dev_acc, ant_tr_acc, ant_dev_acc = eval_model(model)\n",
        "    print('')\n",
        "    return -syn_dev_acc\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "checkpoint_filename = \"{}-checkpoint.pkl\".format(time.asctime())\n",
        "checkpoint_filepath = os.path.join(os.curdir, checkpoint_filename)\n",
        "\n",
        "def backup(res):\n",
        "    skopt.dump(res, checkpoint_filepath)\n",
        "    uploaded = drive.CreateFile({'title': checkpoint_filename})\n",
        "    uploaded.SetContentFile(checkpoint_filepath)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "x0 = [270, 'rms', 256, 30, 0.01, 0.01, 14]\n",
        "res = skopt.gp_minimize(objective, space, x0=x0, n_calls=12, callback=[backup])\n",
        "res.x"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Optimization took 17 seconds\n",
            "* Accuracy on synonym training set: 91.95%\n",
            "* Accuracy on synonym dev set: 84.68%\n",
            "* Accuracy on antonym training set: 10.05%\n",
            "* Accuracy on antonym dev set: 8.66%\n",
            "\n",
            "Uploaded file with ID 1ponF9b9TAu2PvOLNXL4QAiQyyyasPqAQ\n",
            "* Optimization took 16 seconds\n",
            "* Accuracy on synonym training set: 89.41%\n",
            "* Accuracy on synonym dev set: 84.83%\n",
            "* Accuracy on antonym training set: 9.56%\n",
            "* Accuracy on antonym dev set: 9.41%\n",
            "\n",
            "Uploaded file with ID 1pLX53JyN57KEUt14sVXdKCKxd0nZRfD6\n",
            "* Optimization took 11 seconds\n",
            "* Accuracy on synonym training set: 90.62%\n",
            "* Accuracy on synonym dev set: 84.54%\n",
            "* Accuracy on antonym training set: 10.05%\n",
            "* Accuracy on antonym dev set: 9.90%\n",
            "\n",
            "Uploaded file with ID 19OPO04Dfmy2PpUqeVIuoGjmciDMNHQ8z\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on synonym training set: 90.12%\n",
            "* Accuracy on synonym dev set: 84.73%\n",
            "* Accuracy on antonym training set: 10.13%\n",
            "* Accuracy on antonym dev set: 9.90%\n",
            "\n",
            "Uploaded file with ID 1J5b8uxQ0XpOGvcXanKFzO0En2aa6UNJ9\n",
            "* Optimization took 29 seconds\n",
            "* Accuracy on synonym training set: 91.85%\n",
            "* Accuracy on synonym dev set: 84.49%\n",
            "* Accuracy on antonym training set: 10.38%\n",
            "* Accuracy on antonym dev set: 9.16%\n",
            "\n",
            "Uploaded file with ID 1E1GSCpVxMQnN03CyADt1cN6_FASlEEOd\n",
            "* Optimization took 11 seconds\n",
            "* Accuracy on synonym training set: 92.40%\n",
            "* Accuracy on synonym dev set: 84.35%\n",
            "* Accuracy on antonym training set: 10.46%\n",
            "* Accuracy on antonym dev set: 8.91%\n",
            "\n",
            "Uploaded file with ID 15ucx1F5dO4bqO5durVSXBcF_tk79krKL\n",
            "* Optimization took 15 seconds\n",
            "* Accuracy on synonym training set: 89.24%\n",
            "* Accuracy on synonym dev set: 82.99%\n",
            "* Accuracy on antonym training set: 12.19%\n",
            "* Accuracy on antonym dev set: 10.40%\n",
            "\n",
            "Uploaded file with ID 14LvNkRqEo5m73W_ij6VeVRd5fTBhH_UE\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on synonym training set: 92.53%\n",
            "* Accuracy on synonym dev set: 84.61%\n",
            "* Accuracy on antonym training set: 10.54%\n",
            "* Accuracy on antonym dev set: 9.41%\n",
            "\n",
            "Uploaded file with ID 1uYtfA4dbOH4qY-7QmwULHARHZM9By9S1\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on synonym training set: 92.53%\n",
            "* Accuracy on synonym dev set: 84.30%\n",
            "* Accuracy on antonym training set: 10.46%\n",
            "* Accuracy on antonym dev set: 9.65%\n",
            "\n",
            "Uploaded file with ID 1mPWIC1r2a0gd4tBG-mgylqAZa7_P24bB\n",
            "* Optimization took 11 seconds\n",
            "* Accuracy on synonym training set: 92.22%\n",
            "* Accuracy on synonym dev set: 84.47%\n",
            "* Accuracy on antonym training set: 10.63%\n",
            "* Accuracy on antonym dev set: 9.41%\n",
            "\n",
            "Uploaded file with ID 1hqEyuWo7SVC_9tQAaJWiDuk5bBmA7B3t\n",
            "* Optimization took 17 seconds\n",
            "* Accuracy on synonym training set: 92.10%\n",
            "* Accuracy on synonym dev set: 84.18%\n",
            "* Accuracy on antonym training set: 10.46%\n",
            "* Accuracy on antonym dev set: 9.65%\n",
            "\n",
            "Uploaded file with ID 1IXjhg7-VsIbjGa_WeQyM1a5tkLNRyqGN\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on synonym training set: 89.32%\n",
            "* Accuracy on synonym dev set: 84.92%\n",
            "* Accuracy on antonym training set: 9.97%\n",
            "* Accuracy on antonym dev set: 9.65%\n",
            "\n",
            "Uploaded file with ID 1qvowREcoAuIlJ2PzBiMk-Ut_hcxpkjv6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[300, 'rms', 128, 30, 1e-09, 1000.0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "X-UdYDaucr10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}