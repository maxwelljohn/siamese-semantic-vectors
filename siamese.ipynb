{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from scipy.misc import imread\n",
        "\n",
        "!pip install scikit-optimize\n",
        "import skopt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) +\n",
        "                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
        "\n",
        "\n",
        "def similarity(a, b):\n",
        "    a, b = np.ravel(a), np.ravel(b)\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "\n",
        "def create_pairs(x, class_indices, choose_hard_negs=1, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(x))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "        num_outside_family = len(shuffled_indices) - len(family)\n",
        "        assert choose_hard_negs < num_outside_family\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            pairs.append([x[anchor], x[pos]])\n",
        "            labels.append(1)\n",
        "\n",
        "            hardest_neg = None\n",
        "            closest_similarity = float('-infinity')\n",
        "            candidates = 0\n",
        "            while candidates < choose_hard_negs:\n",
        "                try:\n",
        "                    random_neg = shuffled_indices[next_index]\n",
        "                    next_index += 1\n",
        "                    while random_neg in family:\n",
        "                        random_neg = shuffled_indices[next_index]\n",
        "                        next_index += 1\n",
        "                except IndexError:\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                    continue\n",
        "                sim = similarity(x[anchor], x[random_neg])\n",
        "                if sim > closest_similarity:\n",
        "                    hardest_neg = random_neg\n",
        "                    closest_similarity = sim\n",
        "                candidates += 1\n",
        "            pairs.append([x[anchor], x[hardest_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(pairs) == len(labels)\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, layer_size=128, dropout_rate=0.1, layer_count=3):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Flatten()(input)\n",
        "    for i in range(layer_count-1):\n",
        "        x = Dense(layer_size, activation='relu')(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(layer_size, activation='relu')(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    pred = y_pred.ravel() < 0.5\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LOTfhTyQE7ie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_path = get_file('omniglot-background.zip', origin='https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip', extract=True)\n",
        "background_path = os.path.join(os.path.dirname(zip_path), 'images_background')\n",
        "zip_path = get_file('omniglot-evaluation.zip', origin='https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip', extract=True)\n",
        "eval_path = os.path.join(os.path.dirname(zip_path), 'images_evaluation')\n",
        "\n",
        "def load_omniglot_images(path, cutoff=0):\n",
        "    class_num = 0\n",
        "    x = []\n",
        "    y = []\n",
        "    name_for_class_num = []\n",
        "    class_num_for_name = {}\n",
        "    if cutoff == 0:\n",
        "        alphabets = os.listdir(path)\n",
        "    if cutoff > 0:\n",
        "        alphabets = sorted(os.listdir(path))[:cutoff]\n",
        "    else:\n",
        "        alphabets = sorted(os.listdir(path))[cutoff:]\n",
        "    for alphabet in alphabets:\n",
        "        alphabet_path = os.path.join(path, alphabet)\n",
        "        for character in os.listdir(alphabet_path):\n",
        "            name = alphabet + '-' + character\n",
        "            name_for_class_num.append(name)\n",
        "            class_num_for_name[name] = class_num\n",
        "            character_path = os.path.join(alphabet_path, character)\n",
        "            for img in os.listdir(character_path):\n",
        "                img_path = os.path.join(character_path, img)\n",
        "                thumbnail = skimage.transform.resize(imread(img_path), (28, 28))\n",
        "                x.append(thumbnail)\n",
        "                y.append(class_num)\n",
        "            class_num += 1\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x, y, name_for_class_num, class_num_for_name\n",
        "\n",
        "x_train, y_train, name_for_class_num_train, class_num_for_name_train = load_omniglot_images(background_path)\n",
        "# There are 20 evaluation alphabets... we'll split them into two groups of 10.\n",
        "x_dev, y_dev, name_for_class_num_dev, class_num_for_name_dev = load_omniglot_images(eval_path, 10)\n",
        "x_test, y_test, name_for_class_num_test, class_num_for_name_test = load_omniglot_images(eval_path, -10)\n",
        "x_train = x_train.astype('float32')\n",
        "x_dev = x_dev.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_dev /= 255\n",
        "x_test /= 255\n",
        "input_shape = x_train.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CS8v1deGU22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create positive and negative pairs\n",
        "class_indices = [np.where(y_train == i)[0] for i in range(len(name_for_class_num_train))]\n",
        "tr_pairs, tr_y = create_pairs(x_train, class_indices, 1, 30)\n",
        "\n",
        "class_indices = [np.where(y_dev == i)[0] for i in range(len(name_for_class_num_dev))]\n",
        "dev_pairs, dev_y = create_pairs(x_dev, class_indices, 1, 30)\n",
        "\n",
        "class_indices = [np.where(y_test == i)[0] for i in range(len(name_for_class_num_test))]\n",
        "te_pairs, te_y = create_pairs(x_test, class_indices, 1, 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sDnYWX3IT8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Categorical([64, 128, 256, 512], name='layer_size'),\n",
        "    skopt.space.Real(0, 0.5, name='dropout_rate'),\n",
        "    skopt.space.Integer(1, 10, name='layer_count'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([20], name='epochs'),\n",
        "]\n",
        "\n",
        "@skopt.utils.use_named_args(space)\n",
        "def objective(layer_size, dropout_rate, layer_count, optimizer, batch_size, epochs):\n",
        "    print(locals())\n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, layer_size, dropout_rate, layer_count)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(euclidean_distance,\n",
        "                      output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "    \n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_loss, optimizer=opt, metrics=[accuracy])\n",
        "    model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=0,\n",
        "        validation_data=([dev_pairs[:, 0], dev_pairs[:, 1]], dev_y))\n",
        "    \n",
        "    # compute final accuracy on training and test sets\n",
        "    y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
        "    tr_acc = compute_accuracy(tr_y, y_pred)\n",
        "    y_pred = model.predict([dev_pairs[:, 0], dev_pairs[:, 1]])\n",
        "    dev_acc = compute_accuracy(dev_y, y_pred)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "    print('* Accuracy on dev set: %0.2f%%' % (100 * dev_acc))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6GZm2nGjj-E8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = objective([128, 0.1, 3, 'rms', 128, 20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ctewlw7wOxp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
        "te_acc = compute_accuracy(te_y, y_pred)\n",
        "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}