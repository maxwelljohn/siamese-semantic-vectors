{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "outputId": "f13a4997-8db3-4ab3-b781-131bb4cbc3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import heapq\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "import sys\n",
        "import time\n",
        "import unittest\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.constraints import non_neg\n",
        "from scipy.misc import imread\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "!pip install scikit-optimize==0.5.2\n",
        "import skopt\n",
        "\n",
        "!pip install -U -q PyDrive==1.3.1\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.5.2) (1.14.6)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.5.2) (0.20.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.5.2) (1.1.0)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 19.3MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / max(np.linalg.norm(a) * np.linalg.norm(b), sys.float_info.epsilon)\n",
        "\n",
        "\n",
        "def keras_norm(vect):\n",
        "    return K.sqrt(K.batch_dot(vect, vect, axes=1))\n",
        "\n",
        "\n",
        "def keras_similarity(vects):\n",
        "    x, y = vects\n",
        "    # Cosine similarity\n",
        "    return K.batch_dot(x, y, axes=1) / K.maximum(keras_norm(x) * keras_norm(y), K.epsilon())\n",
        "\n",
        "\n",
        "def sim_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, output_size=128, map_reg_rate=0, scale_reg_rate=0):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = input\n",
        "    x = Dense(output_size, kernel_regularizer=regularizers.l2(map_reg_rate))(x)\n",
        "    x = Dense(output_size, kernel_initializer='identity', kernel_constraint=non_neg(),\n",
        "              kernel_regularizer=regularizers.l2(scale_reg_rate), use_bias=False)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def syn_accuracy(y_true, y_pred):\n",
        "    '''Compute synonym classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = y_pred.ravel() > median\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def syn_accuracy_scores(y_true, y_pred):\n",
        "    y_pred = y_pred.ravel()\n",
        "    median = np.median(y_pred)\n",
        "    sd = np.std(y_pred)\n",
        "    standardized = (y_pred - median) / sd\n",
        "    multiplier = y_true * 2 - 1\n",
        "    return standardized * multiplier\n",
        "\n",
        "\n",
        "def ant_accuracy(y_true, y_pred):\n",
        "    '''Compute antonym classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = -((y_pred.ravel() < median).astype(np.int, casting='safe', copy=False))\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def keras_syn_accuracy(y_true, y_pred):\n",
        "    '''Compute synonym classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.75, y_true.dtype)))\n",
        "\n",
        "\n",
        "def keras_ant_accuracy(y_true, y_pred):\n",
        "    '''Compute antonym classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < -0.75, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxHK8Dd1z4k1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip -u glove.6B.zip || (wget http://nlp.stanford.edu/data/glove.6B.zip && unzip -u glove.6B.zip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFVWMt_f0fWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!egrep '^[a-z]+ ' glove.6B.300d.txt > glove.tokens.txt\n",
        "!cut -d' ' -f1 glove.tokens.txt > glove.tokens.strings.txt\n",
        "!cut -d' ' -f2- glove.tokens.txt > glove.tokens.vectors.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2Q9l3PN110t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_strings = np.loadtxt('glove.tokens.strings.txt', dtype=object)\n",
        "word_vectors = np.loadtxt('glove.tokens.vectors.txt')\n",
        "input_shape = word_vectors.shape[1:]\n",
        "assert len(word_strings) == len(word_vectors)\n",
        "\n",
        "index_for_word = {}\n",
        "for i, word in enumerate(word_strings):\n",
        "    index_for_word[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "foWPgSNi-ihu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm *.zip *.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T84gxLzJU9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def might_be_synonyms(w1, w2):\n",
        "    s1 = set()\n",
        "    d1 = set()\n",
        "    s2 = set()\n",
        "    d2 = set()\n",
        "    for synset in wn.synsets(w1):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        s1.update(lemma_names)\n",
        "        d1.update(nltk.word_tokenize(synset.definition()))\n",
        "    for synset in wn.synsets(w2):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        s2.update(lemma_names)\n",
        "        d2.update(nltk.word_tokenize(synset.definition()))\n",
        "    total_intersection = len(s1.intersection(s2)) + len(d1.intersection(s2)) + len(d2.intersection(s1))\n",
        "    return total_intersection > 0\n",
        "\n",
        "\n",
        "class TestSynonyms(unittest.TestCase):\n",
        "    def test_true(self):\n",
        "        pairs = [\n",
        "            ('car', 'auto'),\n",
        "            ('auto', 'car'),\n",
        "            ('car', 'railcar'),\n",
        "            ('small', 'tiny'),\n",
        "            ('small', 'miniature'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertTrue(might_be_synonyms(word1, word2))\n",
        "    def test_false(self):\n",
        "        pairs = [\n",
        "            ('car', 'airplane'),\n",
        "            ('car', 'fast'),\n",
        "            ('small', 'focused'),\n",
        "            ('small', 'accidental'),\n",
        "            ('small', 'rabbit'),\n",
        "            ('small', 'flippant'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertFalse(might_be_synonyms(word1, word2))\n",
        "\n",
        "\n",
        "def get_antonyms(word):\n",
        "    result = set()\n",
        "    for lemma in wn.lemmas(word):\n",
        "        for antonym in lemma.antonyms():\n",
        "            result.add(antonym.name())\n",
        "    return result\n",
        "\n",
        "\n",
        "def might_be_antonyms(w1, w2):\n",
        "    w1_antonyms = get_antonyms(w1)\n",
        "    w2_antonyms = get_antonyms(w2)\n",
        "    for w1_antonym in w1_antonyms:\n",
        "        if might_be_synonyms(w2, w1_antonym):\n",
        "            return True\n",
        "    for w2_antonym in w2_antonyms:\n",
        "        if might_be_synonyms(w1, w2_antonym):\n",
        "            return True\n",
        "\n",
        "\n",
        "class TestAntonyms(unittest.TestCase):\n",
        "    def test_get_antonyms(self):\n",
        "        self.assertEqual(get_antonyms('big'), set(['little', 'small']))\n",
        "        self.assertEqual(get_antonyms('fast'), set(['slow']))\n",
        "        self.assertEqual(get_antonyms('big'), set(['little', 'small']))\n",
        "    def test_true(self):\n",
        "        pairs = [\n",
        "            ('big', 'small'),\n",
        "            ('big', 'minor'),\n",
        "            ('big', 'tiny'),\n",
        "            ('big', 'miniature'),\n",
        "            ('fast', 'slow'),\n",
        "            ('fast', 'sluggish'),\n",
        "            ('loud', 'soft'),\n",
        "            ('loud', 'quiet'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertTrue(might_be_antonyms(word1, word2))\n",
        "    def test_false(self):\n",
        "        pairs = [\n",
        "            ('big', 'huge'),\n",
        "            ('big', 'clean'),\n",
        "            ('fast', 'speedy'),\n",
        "            ('fast', 'unusual'),\n",
        "            ('loud', 'noisy'),\n",
        "            ('loud', 'flat'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertFalse(might_be_antonyms(word1, word2))\n",
        "\n",
        "\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
        "\n",
        "\n",
        "def create_pairs(word_strings, word_vectors, class_indices, pos_label, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    string_pairs = []\n",
        "    vector_pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(word_strings))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            np.random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            string_pairs.append([word_strings[anchor], word_strings[pos]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[pos]])\n",
        "            labels.append(pos_label)\n",
        "            \n",
        "            random_neg = None\n",
        "            while random_neg == None or random_neg in family or \\\n",
        "                    might_be_synonyms(word_strings[anchor], word_strings[random_neg]) or \\\n",
        "                    might_be_antonyms(word_strings[anchor], word_strings[random_neg]):\n",
        "                if next_index >= len(shuffled_indices):\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                random_neg = shuffled_indices[next_index]\n",
        "                next_index += 1\n",
        "\n",
        "            string_pairs.append([word_strings[anchor], word_strings[random_neg]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[random_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(string_pairs) == len(vector_pairs) == len(labels)\n",
        "    return np.array(string_pairs), np.array(vector_pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def create_train_dev_test(word_strings, word_vectors, class_count, word_class, pos_label):\n",
        "    classes = list(range(class_count))\n",
        "    random.seed(850101)\n",
        "    random.shuffle(classes)\n",
        "    random.seed()\n",
        "    train_dev_split = round(len(classes)*0.6)\n",
        "    dev_test_split = round(len(classes)*0.8)\n",
        "    train_classes = classes[:train_dev_split]\n",
        "    dev_classes = classes[train_dev_split:dev_test_split]\n",
        "    test_classes = classes[dev_test_split:]\n",
        "    assert len(set(train_classes).intersection(set(dev_classes))) == 0\n",
        "    assert len(set(dev_classes).intersection(set(test_classes))) == 0\n",
        "    assert len(set(train_classes).intersection(set(test_classes))) == 0\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in train_classes]\n",
        "    tr_strings, tr_pairs, tr_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in dev_classes]\n",
        "    dev_strings, dev_pairs, dev_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in test_classes]\n",
        "    te_strings, te_pairs, te_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "    \n",
        "    return tr_strings, tr_pairs, tr_y, dev_strings, dev_pairs, dev_y, te_strings, te_pairs, te_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6A3h9Wj2Xs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "syn_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_count = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        syn_indices = [index_for_word[syn] for syn in synsets[0].lemma_names() if syn in index_for_word and syn_class[index_for_word[syn]] == -1]\n",
        "        if len(syn_indices) > 1:\n",
        "            syn_class[syn_indices] = class_count\n",
        "            class_count += 1\n",
        "\n",
        "tr_syn_strings, tr_syn_pairs, tr_syn_y, \\\n",
        "dev_syn_strings, dev_syn_pairs, dev_syn_y, \\\n",
        "te_syn_strings, te_syn_pairs, te_syn_y = create_train_dev_test(word_strings, word_vectors, class_count, syn_class, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o13AAwvvaIlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ant_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_count = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    antonyms = get_antonyms(word)\n",
        "    if antonyms:\n",
        "        known_antonyms = [a for a in antonyms if a in index_for_word]\n",
        "        if known_antonyms:\n",
        "            ant_indices = [i, index_for_word[known_antonyms[0]]]\n",
        "            if all(ant_class[ant_indices] == -1):\n",
        "                ant_class[ant_indices] = class_count\n",
        "                class_count += 1\n",
        "\n",
        "tr_ant_strings, tr_ant_pairs, tr_ant_y, \\\n",
        "dev_ant_strings, dev_ant_pairs, dev_ant_y, \\\n",
        "te_ant_strings, te_ant_pairs, te_ant_y = create_train_dev_test(word_strings, word_vectors, class_count, ant_class, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cbbPZrpLSVpd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_model(tr_data, output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff):\n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, output_size, map_reg_rate, scale_reg_rate)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(keras_similarity,\n",
        "                      output_shape=sim_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "\n",
        "    def contrastive_sim_loss(y_true, y_pred):\n",
        "        return K.mean(-y_true * y_pred +\n",
        "                      K.cast(K.equal(y_true, 0), 'float32') * (K.abs(y_pred) ** deviation_dropoff))\n",
        "\n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_sim_loss, optimizer=opt)\n",
        "    \n",
        "    for tr_pairs, tr_y in tr_data:\n",
        "        model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=0)\n",
        "\n",
        "    return model\n",
        "\n",
        "def eval_model(model):\n",
        "    y_pred = model.predict([tr_syn_pairs[:, 0], tr_syn_pairs[:, 1]])\n",
        "    syn_tr_acc = syn_accuracy(tr_syn_y, y_pred)\n",
        "    y_pred = model.predict([dev_syn_pairs[:, 0], dev_syn_pairs[:, 1]])\n",
        "    syn_dev_acc = syn_accuracy(dev_syn_y, y_pred)\n",
        "\n",
        "    y_pred = model.predict([tr_ant_pairs[:, 0], tr_ant_pairs[:, 1]])\n",
        "    ant_tr_acc = ant_accuracy(tr_ant_y, y_pred)\n",
        "    y_pred = model.predict([dev_ant_pairs[:, 0], dev_ant_pairs[:, 1]])\n",
        "    ant_dev_acc = ant_accuracy(dev_ant_y, y_pred)\n",
        "\n",
        "    print('* Accuracy on synonym training set: %0.2f%%' % (100 * syn_tr_acc))\n",
        "    print('* Accuracy on synonym dev set: %0.2f%%' % (100 * syn_dev_acc))\n",
        "    print('* Accuracy on antonym training set: %0.2f%%' % (100 * ant_tr_acc))\n",
        "    print('* Accuracy on antonym dev set: %0.2f%%' % (100 * ant_dev_acc))\n",
        "    return syn_tr_acc, syn_dev_acc, ant_tr_acc, ant_dev_acc\n",
        "\n",
        "def map_word_vectors(word_vectors, model):\n",
        "    base_network = model.layers[2]\n",
        "    l1_weights = base_network.layers[1].get_weights()[0]\n",
        "    l1_biases = base_network.layers[1].get_weights()[1]\n",
        "    l2_weights = base_network.layers[2].get_weights()[0]\n",
        "    mapped = (word_vectors @ l1_weights + l1_biases) @ l2_weights\n",
        "    mapped_2 = (l2_weights.T @ (l1_weights.T @ word_vectors.T + l1_biases.reshape((-1, 1)))).T\n",
        "    assert np.isclose(mapped, mapped_2).all()\n",
        "    return mapped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPptTE4gShhV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_syn_pairs])\n",
        "print('* Baseline synonym accuracy on dev set: %0.2f%%' % (100 * syn_accuracy(dev_syn_y, baseline_similarities)))\n",
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_ant_pairs])\n",
        "print('* Baseline antonym accuracy on dev set: %0.2f%%' % (100 * ant_accuracy(dev_ant_y, baseline_similarities)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l562zedZzM_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Training on synonyms...')\n",
        "syn_model = fit_model([(tr_syn_pairs, tr_syn_y)], 140, 'rms', 256, 30, 1.5e-07, 4e-09, 6)\n",
        "eval_model(syn_model)\n",
        "print('Training on antonyms...')\n",
        "ant_model = fit_model([(tr_ant_pairs, tr_ant_y)], 140, 'rms', 256, 30, 1.5e-07, 4e-09, 6)\n",
        "eval_model(ant_model)\n",
        "print('Training on synonyms & antonyms...')\n",
        "both_model = fit_model([(np.vstack((tr_syn_pairs, tr_ant_pairs)), np.hstack((tr_syn_y, tr_ant_y)))], 140, 'rms', 256, 30, 1.5e-07, 4e-09, 6)\n",
        "eval_model(both_model)\n",
        "print('Training on synonyms, then antonyms...')\n",
        "seq_model = fit_model([(tr_syn_pairs, tr_syn_y), (tr_ant_pairs, tr_ant_y)], 140, 'rms', 256, 30, 1.5e-07, 4e-09, 6)\n",
        "eval_model(seq_model)\n",
        "print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1i_j4YWy2La",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Integer(1, 300, name='output_size'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([30], name='epochs'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='map_reg_rate'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='scale_reg_rate'),\n",
        "    skopt.space.Integer(1, 50, name='deviation_dropoff'),\n",
        "]\n",
        "\n",
        "@skopt.utils.use_named_args(space)\n",
        "def objective(output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff):\n",
        "    start_time = datetime.datetime.now()\n",
        "    model = fit_model([(tr_syn_pairs, tr_syn_y)], output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    syn_tr_acc, syn_dev_acc, ant_tr_acc, ant_dev_acc = eval_model(model)\n",
        "    print('')\n",
        "    return -syn_dev_acc\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "checkpoint_filename = \"{}-checkpoint.pkl\".format(time.asctime())\n",
        "checkpoint_filepath = os.path.join(os.curdir, checkpoint_filename)\n",
        "\n",
        "def backup(res):\n",
        "    skopt.dump(res, checkpoint_filepath)\n",
        "    uploaded = drive.CreateFile({'title': checkpoint_filename})\n",
        "    uploaded.SetContentFile(checkpoint_filepath)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "x0 = [270, 'rms', 256, 30, 0.01, 0.01, 14]\n",
        "res = skopt.gp_minimize(objective, space, x0=x0, n_calls=25, callback=[backup])\n",
        "res.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yce-IhGCnte7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_pair_indices_to_display(word_vectors):\n",
        "    #baseline_sim = np.array([similarity(p[0], p[1]) for p in tr_syn_pairs])\n",
        "    #baseline_scores = syn_accuracy_scores(tr_syn_y, baseline_sim)\n",
        "    optimized_sim = syn_model.predict([tr_syn_pairs[:, 0], tr_syn_pairs[:, 1]])\n",
        "    optimized_scores = syn_accuracy_scores(tr_syn_y, optimized_sim)\n",
        "    display_scores = (tr_syn_y == 1) * optimized_scores\n",
        "    best_for_display = heapq.nlargest(50, zip(display_scores, range(len(display_scores))))\n",
        "    return [index for improvement, index in best_for_display]\n",
        "\n",
        "def plot_synonym_pairs(word_vectors, pairs):\n",
        "    added = set()\n",
        "    strings = []\n",
        "    vectors = []\n",
        "    for word1, word2 in pairs:\n",
        "        if word1 not in added and word2 not in added:\n",
        "            for word in (word1, word2):\n",
        "                added.add(word)\n",
        "                strings.append(word)\n",
        "                vectors.append(word_vectors[index_for_word[word]])\n",
        "\n",
        "    tsne = TSNE(init='pca')\n",
        "    new_values = tsne.fit_transform(vectors)\n",
        "\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    for i in range(0, len(new_values), 2):\n",
        "        random.seed(i * 293778 + 581580)\n",
        "        color = (random.random(), random.random(), random.random())\n",
        "        string1, string2 = strings[i:i+2]\n",
        "        point1, point2 = new_values[i:i+2]\n",
        "        plt.scatter(point1[0], point1[1], c=[color])\n",
        "        plt.annotate(string1,\n",
        "                     xy=(point1[0], point1[1]),\n",
        "                     xytext=(2, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='left',\n",
        "                     va='bottom')\n",
        "        plt.scatter(point2[0], point2[1], c=[color])\n",
        "        plt.annotate(string2,\n",
        "                     xy=(point2[0], point2[1]),\n",
        "                     xytext=(-2, -2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='top')\n",
        "    random.seed()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtAWEeseqOcS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indices_to_display = get_pair_indices_to_display()\n",
        "synonym_pairs = tr_syn_strings[indices_to_display]\n",
        "syn_mapped_word_vectors = map_word_vectors(word_vectors, syn_model)\n",
        "\n",
        "plot_synonym_pairs(word_vectors, synonym_pairs)\n",
        "plot_synonym_pairs(syn_mapped_word_vectors, synonym_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}