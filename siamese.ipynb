{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bf9ed953-e858-43a6-a089-177339de5cc7"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "import sys\n",
        "import time\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.constraints import non_neg\n",
        "from scipy.misc import imread\n",
        "\n",
        "!pip install scikit-optimize\n",
        "import skopt\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    a, b = np.ravel(a), np.ravel(b)\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / max(np.linalg.norm(a) * np.linalg.norm(b), sys.float_info.epsilon)\n",
        "\n",
        "\n",
        "def keras_norm(vect):\n",
        "    return K.sqrt(K.batch_dot(vect, vect, axes=1))\n",
        "\n",
        "\n",
        "def keras_similarity(vects):\n",
        "    x, y = vects\n",
        "    # Cosine similarity\n",
        "    return K.batch_dot(x, y, axes=1) / K.maximum(keras_norm(x) * keras_norm(y), K.epsilon())\n",
        "\n",
        "\n",
        "def sim_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, output_size=128, map_reg_rate=0, scale_reg_rate=0):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = input\n",
        "    x = Dense(output_size, kernel_regularizer=regularizers.l2(map_reg_rate))(x)\n",
        "    x = Dense(output_size, kernel_initializer='identity', kernel_constraint=non_neg(),\n",
        "              kernel_regularizer=regularizers.l2(scale_reg_rate), use_bias=False)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = y_pred.ravel() > median\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def keras_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.75, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxHK8Dd1z4k1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66478079-677b-4924-c26a-6cf62e099597"
      },
      "cell_type": "code",
      "source": [
        "!unzip -u glove.6B.zip || (wget http://nlp.stanford.edu/data/glove.6B.zip && unzip -u glove.6B.zip)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFVWMt_f0fWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 50K words is enough for 95% of English word usage per the OED:\n",
        "# https://web.archive.org/web/20160304170936/http://www.oxforddictionaries.com/words/the-oec-facts-about-the-language\n",
        "# The GloVe text files appear to be roughly sorted by frequency of usage.\n",
        "!egrep '^[a-z]+ ' glove.6B.300d.txt > glove.head.txt\n",
        "!cut -d' ' -f1 glove.head.txt > glove.head.strings.txt\n",
        "!cut -d' ' -f2- glove.head.txt > glove.head.vectors.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2Q9l3PN110t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_strings = np.loadtxt('glove.head.strings.txt', dtype=object)\n",
        "word_vectors = np.loadtxt('glove.head.vectors.txt')\n",
        "input_shape = word_vectors.shape[1:]\n",
        "assert len(word_strings) == len(word_vectors)\n",
        "\n",
        "index_for_word = {}\n",
        "for i, word in enumerate(word_strings):\n",
        "    index_for_word[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T84gxLzJU9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "def are_synonyms(word1, word2):\n",
        "    lemma2 = wnl.lemmatize(word2)\n",
        "    for synset in wn.synsets(word1):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        if word2 in lemma_names or lemma2 in lemma_names:\n",
        "            return True\n",
        "    return False\n",
        "assert are_synonyms('car', 'auto')\n",
        "assert are_synonyms('auto', 'car')\n",
        "assert are_synonyms('car', 'railcar')\n",
        "assert not are_synonyms('car', 'airplane')\n",
        "\n",
        "\n",
        "def create_pairs(word_strings, word_vectors, class_indices, choose_hard_negs=1, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    string_pairs = []\n",
        "    vector_pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(word_strings))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "        num_outside_family = len(shuffled_indices) - len(family)\n",
        "        assert choose_hard_negs < num_outside_family\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            np.random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            string_pairs.append([word_strings[anchor], word_strings[pos]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[pos]])\n",
        "            labels.append(1)\n",
        "\n",
        "            hardest_neg = None\n",
        "            closest_similarity = float('-infinity')\n",
        "            candidates = 0\n",
        "            while candidates < choose_hard_negs:\n",
        "                try:\n",
        "                    random_neg = shuffled_indices[next_index]\n",
        "                    next_index += 1\n",
        "                    while random_neg in family or are_synonyms(word_strings[anchor], word_strings[random_neg]):\n",
        "                        random_neg = shuffled_indices[next_index]\n",
        "                        next_index += 1\n",
        "                except IndexError:\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                    continue\n",
        "                sim = similarity(word_vectors[anchor], word_vectors[random_neg])\n",
        "                if sim > closest_similarity:\n",
        "                    hardest_neg = random_neg\n",
        "                    closest_similarity = sim\n",
        "                candidates += 1\n",
        "            string_pairs.append([word_strings[anchor], word_strings[hardest_neg]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[hardest_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(string_pairs) == len(vector_pairs) == len(labels)\n",
        "    return np.array(string_pairs), np.array(vector_pairs), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6A3h9Wj2Xs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "syn_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_num = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        syn_indices = [index_for_word[syn] for syn in synsets[0].lemma_names() if syn in index_for_word and syn_class[index_for_word[syn]] == -1]\n",
        "        if len(syn_indices) > 1:\n",
        "            syn_class[syn_indices] = class_num\n",
        "            class_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnnybeg0DajU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = list(range(class_num))\n",
        "random.shuffle(classes)\n",
        "train_dev_split = round(len(classes)*0.6)\n",
        "dev_test_split = round(len(classes)*0.8)\n",
        "train_classes = classes[:train_dev_split]\n",
        "dev_classes = classes[train_dev_split:dev_test_split]\n",
        "test_classes = classes[dev_test_split:]\n",
        "assert len(set(train_classes).intersection(set(dev_classes))) == 0\n",
        "assert len(set(dev_classes).intersection(set(test_classes))) == 0\n",
        "assert len(set(train_classes).intersection(set(test_classes))) == 0\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in train_classes]\n",
        "tr_strings, tr_pairs, tr_y = create_pairs(word_strings, word_vectors, class_indices, 5)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in dev_classes]\n",
        "dev_strings, dev_pairs, dev_y = create_pairs(word_strings, word_vectors, class_indices)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in test_classes]\n",
        "te_strings, te_pairs, te_y = create_pairs(word_strings, word_vectors, class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sDnYWX3IT8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Integer(1, 300, name='output_size'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([30], name='epochs'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='map_reg_rate'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='scale_reg_rate'),\n",
        "    skopt.space.Integer(1, 50, name='deviation_dropoff'),\n",
        "]\n",
        "\n",
        "@skopt.utils.use_named_args(space)\n",
        "def objective(output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff):\n",
        "    print(locals())\n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, output_size)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(keras_similarity,\n",
        "                      output_shape=sim_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "\n",
        "    def contrastive_sim_loss(y_true, y_pred):\n",
        "        return K.mean(y_true * (-y_pred + 1) +\n",
        "                      (1 - y_true) * (K.maximum(y_pred, 0) ** deviation_dropoff))\n",
        "    \n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_sim_loss, optimizer=opt, metrics=[keras_accuracy])\n",
        "    model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=0,\n",
        "        validation_data=([dev_pairs[:, 0], dev_pairs[:, 1]], dev_y))\n",
        "    \n",
        "    # compute final accuracy on training and test sets\n",
        "    y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
        "    tr_acc = accuracy(tr_y, y_pred)\n",
        "    y_pred = model.predict([dev_pairs[:, 0], dev_pairs[:, 1]])\n",
        "    dev_acc = accuracy(dev_y, y_pred)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "    print('* Accuracy on dev set: %0.2f%%' % (100 * dev_acc))\n",
        "    \n",
        "    return -dev_acc\n",
        "\n",
        "x0 = [270, 'rms', 256, 30, 0.01, 0.01, 14]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPptTE4gShhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb46590f-a53a-43cb-9e19-b4eedbf1d379"
      },
      "cell_type": "code",
      "source": [
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_pairs])\n",
        "print('* Baseline accuracy on dev set: %0.2f%%' % (100 * accuracy(dev_y, baseline_similarities)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Baseline accuracy on dev set: 77.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p1i_j4YWy2La",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2196
        },
        "outputId": "bdcba1ae-c11d-42c1-dc2a-01a719f86045"
      },
      "cell_type": "code",
      "source": [
        "checkpoint_filename = \"{}-checkpoint.pkl\".format(time.asctime())\n",
        "checkpoint_filepath = os.path.join(os.curdir, checkpoint_filename)\n",
        "\n",
        "def backup(res):\n",
        "    skopt.dump(res, checkpoint_filepath)\n",
        "    uploaded = drive.CreateFile({'title': checkpoint_filename})\n",
        "    uploaded.SetContentFile(checkpoint_filepath)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "res = skopt.gp_minimize(objective, space, x0=x0, n_calls=25, callback=[backup])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'scale_reg_rate': 0.01, 'map_reg_rate': 0.01, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 270, 'deviation_dropoff': 14}\n",
            "* Optimization took 27 seconds\n",
            "* Accuracy on training set: 89.33%\n",
            "* Accuracy on dev set: 84.11%\n",
            "Uploaded file with ID 1jbrSMGRuqyXass8jn9hZHlbtKC5AbLNq\n",
            "{'scale_reg_rate': 9.778344957500946e-09, 'map_reg_rate': 5.7670991626034244e-05, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 176, 'deviation_dropoff': 30}\n",
            "* Optimization took 17 seconds\n",
            "* Accuracy on training set: 89.56%\n",
            "* Accuracy on dev set: 84.01%\n",
            "Uploaded file with ID 1GST_kK5W6o0nOD02DW8ecXa1WMJnj3cG\n",
            "{'scale_reg_rate': 0.0002578845453366895, 'map_reg_rate': 3.365753869765262e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 70, 'deviation_dropoff': 45}\n",
            "* Optimization took 17 seconds\n",
            "* Accuracy on training set: 90.03%\n",
            "* Accuracy on dev set: 83.06%\n",
            "Uploaded file with ID 1LpjKDP1R0VfroElaikst-J_JFc3yjS5M\n",
            "{'scale_reg_rate': 2.167327587686808e-08, 'map_reg_rate': 4.210715018285538e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 51, 'deviation_dropoff': 49}\n",
            "* Optimization took 17 seconds\n",
            "* Accuracy on training set: 89.57%\n",
            "* Accuracy on dev set: 83.75%\n",
            "Uploaded file with ID 1tqqnoXW0ahjb-SsLFQM-NNvmfGupaAsD\n",
            "{'scale_reg_rate': 342.46598211664553, 'map_reg_rate': 7.982490724270758e-05, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 46, 'deviation_dropoff': 27}\n",
            "* Optimization took 17 seconds\n",
            "* Accuracy on training set: 89.28%\n",
            "* Accuracy on dev set: 83.78%\n",
            "Uploaded file with ID 1xPtBxUd589ORZE67of-cWGH0rVnyobIg\n",
            "{'scale_reg_rate': 10.301491496620105, 'map_reg_rate': 4.0432299064554234e-08, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 112, 'deviation_dropoff': 38}\n",
            "* Optimization took 47 seconds\n",
            "* Accuracy on training set: 90.13%\n",
            "* Accuracy on dev set: 83.67%\n",
            "Uploaded file with ID 1_HJ3dqdyQmGinYKxM9WlqZiqgx4srf6t\n",
            "{'scale_reg_rate': 0.41804264034036714, 'map_reg_rate': 7.97353846842104e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 235, 'deviation_dropoff': 40}\n",
            "* Optimization took 17 seconds\n",
            "* Accuracy on training set: 90.13%\n",
            "* Accuracy on dev set: 83.42%\n",
            "Uploaded file with ID 1lo8BPtPfBfOn0YllJUBxCssMx2buGcwb\n",
            "{'scale_reg_rate': 3.43121801054129e-05, 'map_reg_rate': 9.173980938998038e-06, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 69, 'deviation_dropoff': 10}\n",
            "* Optimization took 27 seconds\n",
            "* Accuracy on training set: 88.84%\n",
            "* Accuracy on dev set: 84.24%\n",
            "Uploaded file with ID 1ze80zCcpPrJ_EJlh40POSCAsl-xpf_un\n",
            "{'scale_reg_rate': 1.3201215159683216e-05, 'map_reg_rate': 9.620749888074729e-07, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 251, 'deviation_dropoff': 46}\n",
            "* Optimization took 48 seconds\n",
            "* Accuracy on training set: 90.24%\n",
            "* Accuracy on dev set: 83.24%\n",
            "Uploaded file with ID 13QfKb73e1UFkhXBu8TW1R7zvyo4vqeUD\n",
            "{'scale_reg_rate': 3.434324759309362e-08, 'map_reg_rate': 0.0014717114868147351, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 253, 'deviation_dropoff': 46}\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on training set: 90.38%\n",
            "* Accuracy on dev set: 83.80%\n",
            "Uploaded file with ID 1bdexqBQefgi3697VRVB6rJ_z4Nrtj_4j\n",
            "{'scale_reg_rate': 0.007758086981163041, 'map_reg_rate': 0.00013531374367984313, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 187, 'deviation_dropoff': 31}\n",
            "* Optimization took 48 seconds\n",
            "* Accuracy on training set: 89.06%\n",
            "* Accuracy on dev set: 83.62%\n",
            "Uploaded file with ID 1cjxZp3_l50cDfztetd9kC0JZZJM0Z_cE\n",
            "{'scale_reg_rate': 1.3609549853415773e-09, 'map_reg_rate': 73.22788118351423, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 166, 'deviation_dropoff': 1}\n",
            "* Optimization took 18 seconds\n",
            "* Accuracy on training set: 82.41%\n",
            "* Accuracy on dev set: 80.51%\n",
            "Uploaded file with ID 1bRV3_ESxqbBaTG1_s7H4sp4PcTf_qzyr\n",
            "{'scale_reg_rate': 1000.0, 'map_reg_rate': 8.668306852204737e-06, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 1, 'deviation_dropoff': 1}\n",
            "* Optimization took 27 seconds\n",
            "* Accuracy on training set: 50.00%\n",
            "* Accuracy on dev set: 50.00%\n",
            "Uploaded file with ID 1jUx8_jia3pzrKynd10q2DXMYY1CEYTFh\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 1, 'deviation_dropoff': 1}\n",
            "* Optimization took 27 seconds\n",
            "* Accuracy on training set: 50.00%\n",
            "* Accuracy on dev set: 50.00%\n",
            "Uploaded file with ID 1spjGreDb8ZmxdGHAVIFIXZ7xFoCE-Gix\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 50}\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on training set: 90.21%\n",
            "* Accuracy on dev set: 83.93%\n",
            "Uploaded file with ID 1zhjBtKk9EBL7Jax0YLiEbKkTv7fni0fR\n",
            "{'scale_reg_rate': 1000.0, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 55, 'deviation_dropoff': 1}\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on training set: 82.13%\n",
            "* Accuracy on dev set: 80.53%\n",
            "Uploaded file with ID 1WQDiIL1TWEfVlKM3MevCSVzYDPs-Rrwk\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 1}\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on training set: 82.27%\n",
            "* Accuracy on dev set: 80.30%\n",
            "Uploaded file with ID 1LzoLUYB8C5Z6AzNy2nZPBdd2MxLo6cya\n",
            "{'scale_reg_rate': 4.219442002702701e-08, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 286, 'deviation_dropoff': 50}\n",
            "* Optimization took 29 seconds\n",
            "* Accuracy on training set: 89.82%\n",
            "* Accuracy on dev set: 83.55%\n",
            "Uploaded file with ID 1ube_kzzOsYeqSleB91-CcCzy6sw_8xcb\n",
            "{'scale_reg_rate': 3.4445163729674615, 'map_reg_rate': 0.09980594739208576, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 84, 'deviation_dropoff': 6}\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on training set: 88.03%\n",
            "* Accuracy on dev set: 84.13%\n",
            "Uploaded file with ID 11mD4EsSwM-FTILZCqhd_irQN8BLze4Q5\n",
            "{'scale_reg_rate': 1000.0, 'map_reg_rate': 9.470496979703497e-08, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 50}\n",
            "* Optimization took 29 seconds\n",
            "* Accuracy on training set: 89.65%\n",
            "* Accuracy on dev set: 83.52%\n",
            "Uploaded file with ID 1tKdUszCd9_wn3ZmsjNnZGlcK-bTU2eJo\n",
            "{'scale_reg_rate': 2.8328275650924e-06, 'map_reg_rate': 999.4312081732534, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 1, 'deviation_dropoff': 50}\n",
            "* Optimization took 48 seconds\n",
            "* Accuracy on training set: 50.00%\n",
            "* Accuracy on dev set: 50.00%\n",
            "Uploaded file with ID 15Laf61n1Se10yYC-q4QDOXB8jh4wtRd7\n",
            "{'scale_reg_rate': 9.29543074697053e-09, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 187, 'deviation_dropoff': 50}\n",
            "* Optimization took 18 seconds\n",
            "* Accuracy on training set: 90.29%\n",
            "* Accuracy on dev set: 83.01%\n",
            "Uploaded file with ID 1TjyGvGAUzvLL9lHx_8WQX2XNPtqzjlM4\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 274, 'deviation_dropoff': 50}\n",
            "* Optimization took 29 seconds\n",
            "* Accuracy on training set: 90.01%\n",
            "* Accuracy on dev set: 83.78%\n",
            "Uploaded file with ID 10leyASAUBpUIU7T7YuxXVIWPSQh8slnZ\n",
            "{'scale_reg_rate': 1000.0, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 213, 'deviation_dropoff': 1}\n",
            "* Optimization took 28 seconds\n",
            "* Accuracy on training set: 82.13%\n",
            "* Accuracy on dev set: 80.48%\n",
            "Uploaded file with ID 13UhI0H_iF0iVbYAogmrfknROrWJMOpaW\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 50}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skopt/optimizer/optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
            "  warnings.warn(\"The objective has been evaluated \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "* Optimization took 29 seconds\n",
            "* Accuracy on training set: 90.32%\n",
            "* Accuracy on dev set: 83.06%\n",
            "Uploaded file with ID 1Q50HK8FFfnaKzkzdM0ktm9pGshS-7EC4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qoUX1lm4MxMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffaa8f99-0beb-4940-d05e-2180f7d83974"
      },
      "cell_type": "code",
      "source": [
        "res.x"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[69, 'rms', 256, 30, 9.173980938998038e-06, 3.43121801054129e-05, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "z-wcc2Yfolmo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}