{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "dc889e83-4eac-434c-993e-0c93fb348987"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "import sys\n",
        "import time\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.constraints import non_neg\n",
        "from scipy.misc import imread\n",
        "\n",
        "!pip install scikit-optimize\n",
        "import skopt\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    a, b = np.ravel(a), np.ravel(b)\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / max(np.linalg.norm(a) * np.linalg.norm(b), sys.float_info.epsilon)\n",
        "\n",
        "\n",
        "def keras_norm(vect):\n",
        "    return K.sqrt(K.batch_dot(vect, vect, axes=1))\n",
        "\n",
        "\n",
        "def keras_similarity(vects):\n",
        "    x, y = vects\n",
        "    # Cosine similarity\n",
        "    return K.batch_dot(x, y, axes=1) / K.maximum(keras_norm(x) * keras_norm(y), K.epsilon())\n",
        "\n",
        "\n",
        "def sim_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, output_size=128, reg_rate=0):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = input\n",
        "    x = Dense(output_size, kernel_regularizer=regularizers.l2(reg_rate), use_bias=False)(x)\n",
        "    x = Dense(output_size, kernel_initializer='identity', kernel_constraint=non_neg(),\n",
        "              kernel_regularizer=regularizers.l2(reg_rate), use_bias=False)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = y_pred.ravel() > median\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def keras_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.75, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxHK8Dd1z4k1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56771caf-53fc-4caa-d00d-7a0eb129a15a"
      },
      "cell_type": "code",
      "source": [
        "!unzip -u glove.6B.zip || (wget http://nlp.stanford.edu/data/glove.6B.zip && unzip -u glove.6B.zip)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFVWMt_f0fWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 50K words is enough for 95% of English word usage per the OED:\n",
        "# https://web.archive.org/web/20160304170936/http://www.oxforddictionaries.com/words/the-oec-facts-about-the-language\n",
        "# The GloVe text files appear to be roughly sorted by frequency of usage.\n",
        "!egrep '^[a-z]+ ' glove.6B.300d.txt > glove.head.txt\n",
        "!cut -d' ' -f1 glove.head.txt > glove.head.strings.txt\n",
        "!cut -d' ' -f2- glove.head.txt > glove.head.vectors.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2Q9l3PN110t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_strings = np.loadtxt('glove.head.strings.txt', dtype=object)\n",
        "word_vectors = np.loadtxt('glove.head.vectors.txt')\n",
        "input_shape = word_vectors.shape[1:]\n",
        "assert len(word_strings) == len(word_vectors)\n",
        "\n",
        "index_for_word = {}\n",
        "for i, word in enumerate(word_strings):\n",
        "    index_for_word[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T84gxLzJU9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "def are_synonyms(word1, word2):\n",
        "    lemma2 = wnl.lemmatize(word2)\n",
        "    for synset in wn.synsets(word1):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        if word2 in lemma_names or lemma2 in lemma_names:\n",
        "            return True\n",
        "    return False\n",
        "assert are_synonyms('car', 'auto')\n",
        "assert are_synonyms('auto', 'car')\n",
        "assert are_synonyms('car', 'railcar')\n",
        "assert not are_synonyms('car', 'airplane')\n",
        "\n",
        "\n",
        "def create_pairs(word_strings, word_vectors, class_indices, choose_hard_negs=1, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    string_pairs = []\n",
        "    vector_pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(word_strings))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "        num_outside_family = len(shuffled_indices) - len(family)\n",
        "        assert choose_hard_negs < num_outside_family\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            np.random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            string_pairs.append([word_strings[anchor], word_strings[pos]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[pos]])\n",
        "            labels.append(1)\n",
        "\n",
        "            hardest_neg = None\n",
        "            closest_similarity = float('-infinity')\n",
        "            candidates = 0\n",
        "            while candidates < choose_hard_negs:\n",
        "                try:\n",
        "                    random_neg = shuffled_indices[next_index]\n",
        "                    next_index += 1\n",
        "                    while random_neg in family or are_synonyms(word_strings[anchor], word_strings[random_neg]):\n",
        "                        random_neg = shuffled_indices[next_index]\n",
        "                        next_index += 1\n",
        "                except IndexError:\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                    continue\n",
        "                sim = similarity(word_vectors[anchor], word_vectors[random_neg])\n",
        "                if sim > closest_similarity:\n",
        "                    hardest_neg = random_neg\n",
        "                    closest_similarity = sim\n",
        "                candidates += 1\n",
        "            string_pairs.append([word_strings[anchor], word_strings[hardest_neg]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[hardest_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(string_pairs) == len(vector_pairs) == len(labels)\n",
        "    return np.array(string_pairs), np.array(vector_pairs), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6A3h9Wj2Xs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "syn_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_num = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        syn_indices = [index_for_word[syn] for syn in synsets[0].lemma_names() if syn in index_for_word and syn_class[index_for_word[syn]] == -1]\n",
        "        if len(syn_indices) > 1:\n",
        "            syn_class[syn_indices] = class_num\n",
        "            class_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnnybeg0DajU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = list(range(class_num))\n",
        "random.shuffle(classes)\n",
        "train_dev_split = round(len(classes)*0.6)\n",
        "dev_test_split = round(len(classes)*0.8)\n",
        "train_classes = classes[:train_dev_split]\n",
        "dev_classes = classes[train_dev_split:dev_test_split]\n",
        "test_classes = classes[dev_test_split:]\n",
        "assert len(set(train_classes).intersection(set(dev_classes))) == 0\n",
        "assert len(set(dev_classes).intersection(set(test_classes))) == 0\n",
        "assert len(set(train_classes).intersection(set(test_classes))) == 0\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in train_classes]\n",
        "tr_strings, tr_pairs, tr_y = create_pairs(word_strings, word_vectors, class_indices)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in dev_classes]\n",
        "dev_strings, dev_pairs, dev_y = create_pairs(word_strings, word_vectors, class_indices)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in test_classes]\n",
        "te_strings, te_pairs, te_y = create_pairs(word_strings, word_vectors, class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sDnYWX3IT8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Integer(1, 300, name='output_size'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([20], name='epochs'),\n",
        "    skopt.space.Real(0.00001, 10, prior='log-uniform', name='reg_rate'),\n",
        "    skopt.space.Integer(1, 50, name='deviation_dropoff'),\n",
        "]\n",
        "\n",
        "@skopt.utils.use_named_args(space)\n",
        "def objective(output_size, optimizer, batch_size, epochs, reg_rate, deviation_dropoff):\n",
        "    print(locals())\n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, output_size)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(keras_similarity,\n",
        "                      output_shape=sim_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "\n",
        "    def contrastive_sim_loss(y_true, y_pred):\n",
        "        return K.mean(y_true * (-y_pred + 1) +\n",
        "                      (1 - y_true) * (K.maximum(y_pred, 0) ** deviation_dropoff))\n",
        "    \n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_sim_loss, optimizer=opt, metrics=[keras_accuracy])\n",
        "    model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=0,\n",
        "        validation_data=([dev_pairs[:, 0], dev_pairs[:, 1]], dev_y))\n",
        "    \n",
        "    # compute final accuracy on training and test sets\n",
        "    y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
        "    tr_acc = accuracy(tr_y, y_pred)\n",
        "    y_pred = model.predict([dev_pairs[:, 0], dev_pairs[:, 1]])\n",
        "    dev_acc = accuracy(dev_y, y_pred)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "    print('* Accuracy on dev set: %0.2f%%' % (100 * dev_acc))\n",
        "    \n",
        "    return -dev_acc\n",
        "\n",
        "x0 = [128, 'rms', 512, 20, 0.00001, 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPptTE4gShhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c9551d6-79d0-472f-b330-d0bf2b27843b"
      },
      "cell_type": "code",
      "source": [
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_pairs])\n",
        "print('* Baseline accuracy on dev set: %0.2f%%' % (100 * accuracy(dev_y, baseline_similarities)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Baseline accuracy on dev set: 75.57%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p1i_j4YWy2La",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2142
        },
        "outputId": "b0f71fd6-3ddf-4d24-9e84-659d2f123bdd"
      },
      "cell_type": "code",
      "source": [
        "checkpoint_filename = \"{}-checkpoint.pkl\".format(time.asctime())\n",
        "checkpoint_filepath = os.path.join(os.curdir, checkpoint_filename)\n",
        "\n",
        "def backup(res):\n",
        "    skopt.dump(res, checkpoint_filepath)\n",
        "    uploaded = drive.CreateFile({'title': checkpoint_filename})\n",
        "    uploaded.SetContentFile(checkpoint_filepath)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "res = skopt.gp_minimize(objective, space, x0=x0, n_calls=25, callback=[backup])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'reg_rate': 1e-05, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 128, 'deviation_dropoff': 2}\n",
            "* Optimization took 14 seconds\n",
            "* Accuracy on training set: 86.35%\n",
            "* Accuracy on dev set: 81.36%\n",
            "Uploaded file with ID 1HxjiL14JeoAjI8LmXGfpTsZuP6Yhg00l\n",
            "{'reg_rate': 1.0847553269175899e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 186, 'deviation_dropoff': 29}\n",
            "* Optimization took 33 seconds\n",
            "* Accuracy on training set: 84.96%\n",
            "* Accuracy on dev set: 78.01%\n",
            "Uploaded file with ID 1c-M4pLSvc7SYA9KSm_UG7V1D0mMXu5SS\n",
            "{'reg_rate': 0.00020398359343191453, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 197, 'deviation_dropoff': 34}\n",
            "* Optimization took 35 seconds\n",
            "* Accuracy on training set: 84.63%\n",
            "* Accuracy on dev set: 78.03%\n",
            "Uploaded file with ID 1WuT4XCChiLy0HdgvcEOL_tKP-UOS8PaX\n",
            "{'reg_rate': 4.8033298926932604e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 137, 'deviation_dropoff': 31}\n",
            "* Optimization took 34 seconds\n",
            "* Accuracy on training set: 84.25%\n",
            "* Accuracy on dev set: 77.34%\n",
            "Uploaded file with ID 1jGGFWLLvF459jPS8Q6_Rugh2y8pyCnhd\n",
            "{'reg_rate': 1.1441369591630024e-05, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 245, 'deviation_dropoff': 37}\n",
            "* Optimization took 15 seconds\n",
            "* Accuracy on training set: 83.81%\n",
            "* Accuracy on dev set: 77.75%\n",
            "Uploaded file with ID 1BeJz8W6GbX4UhI6JwCSMGSPLGkGvkzfS\n",
            "{'reg_rate': 3.248603514239237, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 98, 'deviation_dropoff': 10}\n",
            "* Optimization took 14 seconds\n",
            "* Accuracy on training set: 85.64%\n",
            "* Accuracy on dev set: 79.52%\n",
            "Uploaded file with ID 1clGERWReHiUDuvVNvec2Q2bU0f9PqJgI\n",
            "{'reg_rate': 2.207753245456806e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 179, 'deviation_dropoff': 24}\n",
            "* Optimization took 34 seconds\n",
            "* Accuracy on training set: 85.03%\n",
            "* Accuracy on dev set: 78.21%\n",
            "Uploaded file with ID 1vv5mOmnB-2LQVCuMEtn20bMUNvZvX1qm\n",
            "{'reg_rate': 0.04914497688614981, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 39, 'deviation_dropoff': 17}\n",
            "* Optimization took 36 seconds\n",
            "* Accuracy on training set: 85.51%\n",
            "* Accuracy on dev set: 78.49%\n",
            "Uploaded file with ID 1VrxkvNoHrFVP-OqOpHaMVGKeIVsk02Mn\n",
            "{'reg_rate': 1.5833534265949574, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 59, 'deviation_dropoff': 41}\n",
            "* Optimization took 13 seconds\n",
            "* Accuracy on training set: 82.81%\n",
            "* Accuracy on dev set: 77.30%\n",
            "Uploaded file with ID 1yEOYI4zp4di40IKHI-b5s2CCeVbzlOdO\n",
            "{'reg_rate': 4.782084201156765, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 99, 'deviation_dropoff': 14}\n",
            "* Optimization took 15 seconds\n",
            "* Accuracy on training set: 85.40%\n",
            "* Accuracy on dev set: 79.38%\n",
            "Uploaded file with ID 1LnWDqBkg7NJ3wT6ATB34knKmCc8LNZGr\n",
            "{'reg_rate': 1.8906604704934694e-05, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 6, 'deviation_dropoff': 19}\n",
            "* Optimization took 15 seconds\n",
            "* Accuracy on training set: 82.41%\n",
            "* Accuracy on dev set: 77.92%\n",
            "Uploaded file with ID 1jo_bs0PCWI37EZIGCRKgJZPJEfuqf3Rp\n",
            "{'reg_rate': 10.0, 'epochs': 20, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 1}\n",
            "* Optimization took 22 seconds\n",
            "* Accuracy on training set: 85.60%\n",
            "* Accuracy on dev set: 80.58%\n",
            "Uploaded file with ID 1sslru0Zi3ADFlrEztp8tRdp3KTNvHPGC\n",
            "{'reg_rate': 1.1018482808146861e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 1}\n",
            "* Optimization took 36 seconds\n",
            "* Accuracy on training set: 85.58%\n",
            "* Accuracy on dev set: 81.01%\n",
            "Uploaded file with ID 1wYY2kXA8hWBaIxV7Q1Ue9Seb88P0NFRr\n",
            "{'reg_rate': 10.0, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 1}\n",
            "* Optimization took 15 seconds\n",
            "* Accuracy on training set: 85.48%\n",
            "* Accuracy on dev set: 81.18%\n",
            "Uploaded file with ID 1LzPzvR64IoYUNBDjxuRnrmfKtzz3tgKd\n",
            "{'reg_rate': 1e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 1, 'deviation_dropoff': 1}\n",
            "* Optimization took 32 seconds\n",
            "* Accuracy on training set: 50.00%\n",
            "* Accuracy on dev set: 50.00%\n",
            "Uploaded file with ID 1IqF3_S65H8GDE4ustxYPyNVXt5USAz88\n",
            "{'reg_rate': 1.5504039194834165, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 75, 'deviation_dropoff': 27}\n",
            "* Optimization took 37 seconds\n",
            "* Accuracy on training set: 84.52%\n",
            "* Accuracy on dev set: 77.32%\n",
            "Uploaded file with ID 1yXaWe6_CT3RxhBpO9ZsnJoELZoo3UFcd\n",
            "{'reg_rate': 0.05902707873994604, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 199, 'deviation_dropoff': 5}\n",
            "* Optimization took 15 seconds\n",
            "* Accuracy on training set: 86.59%\n",
            "* Accuracy on dev set: 81.13%\n",
            "Uploaded file with ID 167c-loG1_fNG3mFuFLyEJh0PPpbr3_-g\n",
            "{'reg_rate': 10.0, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 20}\n",
            "* Optimization took 35 seconds\n",
            "* Accuracy on training set: 85.54%\n",
            "* Accuracy on dev set: 79.29%\n",
            "Uploaded file with ID 1T5uOIhUIW5suz9jV4awVCdiAmdWdaxgR\n",
            "{'reg_rate': 6.1165223094932495, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 205, 'deviation_dropoff': 1}\n",
            "* Optimization took 16 seconds\n",
            "* Accuracy on training set: 85.35%\n",
            "* Accuracy on dev set: 80.88%\n",
            "Uploaded file with ID 1T6GbU0-AspgWfN_I7ex_AFcnm4F96WEr\n",
            "{'reg_rate': 1e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 285, 'deviation_dropoff': 1}\n",
            "* Optimization took 34 seconds\n",
            "* Accuracy on training set: 85.49%\n",
            "* Accuracy on dev set: 80.79%\n",
            "Uploaded file with ID 1lawIvQ2Ps9gfQQFndudq_YAcJeA6SCcC\n",
            "{'reg_rate': 0.0013003289091327993, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 124, 'deviation_dropoff': 12}\n",
            "* Optimization took 16 seconds\n",
            "* Accuracy on training set: 85.29%\n",
            "* Accuracy on dev set: 79.78%\n",
            "Uploaded file with ID 1RBL2mD5RsbIujB4QkfJk5WTF1kMwHHpr\n",
            "{'reg_rate': 0.013696233402824857, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 1}\n",
            "* Optimization took 16 seconds\n",
            "* Accuracy on training set: 85.29%\n",
            "* Accuracy on dev set: 80.88%\n",
            "Uploaded file with ID 1lgTkll4EbVZSEZaLcjWuVPAiq2QDWLAP\n",
            "{'reg_rate': 3.0385362191163096e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 7}\n",
            "* Optimization took 35 seconds\n",
            "* Accuracy on training set: 86.42%\n",
            "* Accuracy on dev set: 80.49%\n",
            "Uploaded file with ID 1iBUEyLnOsZXgHZ_YbGrD-Lt92AsJ7_Ua\n",
            "{'reg_rate': 1e-05, 'epochs': 20, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 144, 'deviation_dropoff': 1}\n",
            "* Optimization took 16 seconds\n",
            "* Accuracy on training set: 85.64%\n",
            "* Accuracy on dev set: 80.62%\n",
            "Uploaded file with ID 1OG_EKTcKLkc3o15xOZYtfISJrUuKgQO4\n",
            "{'reg_rate': 1e-05, 'epochs': 20, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 292, 'deviation_dropoff': 1}\n",
            "* Optimization took 37 seconds\n",
            "* Accuracy on training set: 85.48%\n",
            "* Accuracy on dev set: 81.08%\n",
            "Uploaded file with ID 1_E2TSy1hDvmb314rlQS4fVtsI4IPQKDE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qoUX1lm4MxMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "300da6a2-b9ba-4ba6-f7b7-17016412cb07"
      },
      "cell_type": "code",
      "source": [
        "res.x"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[128, 'rms', 512, 20, 1e-05, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "z-wcc2Yfolmo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}