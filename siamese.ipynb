{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "import sys\n",
        "import time\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from scipy.misc import imread\n",
        "\n",
        "!pip install scikit-optimize\n",
        "import skopt\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    a, b = np.ravel(a), np.ravel(b)\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / max(np.linalg.norm(a) * np.linalg.norm(b), sys.float_info.epsilon)\n",
        "\n",
        "\n",
        "def keras_norm(vect):\n",
        "    return K.sqrt(K.batch_dot(vect, vect, axes=1))\n",
        "\n",
        "\n",
        "def keras_similarity(vects):\n",
        "    x, y = vects\n",
        "    # Cosine similarity\n",
        "    return K.batch_dot(x, y, axes=1) / K.maximum(keras_norm(x) * keras_norm(y), K.epsilon())\n",
        "\n",
        "\n",
        "def sim_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, output_size=128, reg_rate=0):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = input\n",
        "    x = Dense(output_size, kernel_regularizer=regularizers.l2(reg_rate))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = y_pred.ravel() > median\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def keras_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.75, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxHK8Dd1z4k1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip -u glove.6B.zip || (wget http://nlp.stanford.edu/data/glove.6B.zip && unzip -u glove.6B.zip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFVWMt_f0fWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 50K words is enough for 95% of English word usage per the OED:\n",
        "# https://web.archive.org/web/20160304170936/http://www.oxforddictionaries.com/words/the-oec-facts-about-the-language\n",
        "# The GloVe text files appear to be roughly sorted by frequency of usage.\n",
        "!egrep '^[a-z]+ ' glove.6B.300d.txt > glove.head.txt\n",
        "!cut -d' ' -f1 glove.head.txt > glove.head.strings.txt\n",
        "!cut -d' ' -f2- glove.head.txt > glove.head.vectors.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2Q9l3PN110t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_strings = np.loadtxt('glove.head.strings.txt', dtype=object)\n",
        "word_vectors = np.loadtxt('glove.head.vectors.txt')\n",
        "input_shape = word_vectors.shape[1:]\n",
        "assert len(word_strings) == len(word_vectors)\n",
        "\n",
        "index_for_word = {}\n",
        "for i, word in enumerate(word_strings):\n",
        "    index_for_word[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T84gxLzJU9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "def are_synonyms(word1, word2):\n",
        "    lemma2 = wnl.lemmatize(word2)\n",
        "    for synset in wn.synsets(word1):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        if word2 in lemma_names or lemma2 in lemma_names:\n",
        "            return True\n",
        "    return False\n",
        "assert are_synonyms('car', 'auto')\n",
        "assert are_synonyms('auto', 'car')\n",
        "assert are_synonyms('car', 'railcar')\n",
        "assert not are_synonyms('car', 'airplane')\n",
        "\n",
        "\n",
        "def create_pairs(word_strings, word_vectors, class_indices, choose_hard_negs=1, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    string_pairs = []\n",
        "    vector_pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(word_strings))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "        num_outside_family = len(shuffled_indices) - len(family)\n",
        "        assert choose_hard_negs < num_outside_family\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            np.random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            string_pairs.append([word_strings[anchor], word_strings[pos]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[pos]])\n",
        "            labels.append(1)\n",
        "\n",
        "            hardest_neg = None\n",
        "            closest_similarity = float('-infinity')\n",
        "            candidates = 0\n",
        "            while candidates < choose_hard_negs:\n",
        "                try:\n",
        "                    random_neg = shuffled_indices[next_index]\n",
        "                    next_index += 1\n",
        "                    while random_neg in family or are_synonyms(word_strings[anchor], word_strings[random_neg]):\n",
        "                        random_neg = shuffled_indices[next_index]\n",
        "                        next_index += 1\n",
        "                except IndexError:\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                    continue\n",
        "                sim = similarity(word_vectors[anchor], word_vectors[random_neg])\n",
        "                if sim > closest_similarity:\n",
        "                    hardest_neg = random_neg\n",
        "                    closest_similarity = sim\n",
        "                candidates += 1\n",
        "            string_pairs.append([word_strings[anchor], word_strings[hardest_neg]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[hardest_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(string_pairs) == len(vector_pairs) == len(labels)\n",
        "    return np.array(string_pairs), np.array(vector_pairs), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6A3h9Wj2Xs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "syn_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_num = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        syn_indices = [index_for_word[syn] for syn in synsets[0].lemma_names() if syn in index_for_word and syn_class[index_for_word[syn]] == -1]\n",
        "        if len(syn_indices) > 1:\n",
        "            syn_class[syn_indices] = class_num\n",
        "            class_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnnybeg0DajU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = list(range(class_num))\n",
        "random.shuffle(classes)\n",
        "train_dev_split = round(len(classes)*0.6)\n",
        "dev_test_split = round(len(classes)*0.8)\n",
        "train_classes = classes[:train_dev_split]\n",
        "dev_classes = classes[train_dev_split:dev_test_split]\n",
        "test_classes = classes[dev_test_split:]\n",
        "assert len(set(train_classes).intersection(set(dev_classes))) == 0\n",
        "assert len(set(dev_classes).intersection(set(test_classes))) == 0\n",
        "assert len(set(train_classes).intersection(set(test_classes))) == 0\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in train_classes]\n",
        "tr_strings, tr_pairs, tr_y = create_pairs(word_strings, word_vectors, class_indices)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in dev_classes]\n",
        "dev_strings, dev_pairs, dev_y = create_pairs(word_strings, word_vectors, class_indices)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in test_classes]\n",
        "te_strings, te_pairs, te_y = create_pairs(word_strings, word_vectors, class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sDnYWX3IT8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Integer(1, 300, name='output_size'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([20], name='epochs'),\n",
        "    skopt.space.Real(0.00001, 10, prior='log-uniform', name='reg_rate'),\n",
        "    skopt.space.Integer(1, 50, name='deviation_dropoff'),\n",
        "]\n",
        "\n",
        "@skopt.utils.use_named_args(space)\n",
        "def objective(output_size, optimizer, batch_size, epochs, reg_rate, deviation_dropoff):\n",
        "    print(locals())\n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, output_size)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(keras_similarity,\n",
        "                      output_shape=sim_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "\n",
        "    def contrastive_sim_loss(y_true, y_pred):\n",
        "        return K.mean(y_true * (-y_pred + 1) +\n",
        "                      (1 - y_true) * (K.maximum(y_pred, 0) ** deviation_dropoff))\n",
        "    \n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_sim_loss, optimizer=opt, metrics=[keras_accuracy])\n",
        "    model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=0,\n",
        "        validation_data=([dev_pairs[:, 0], dev_pairs[:, 1]], dev_y))\n",
        "    \n",
        "    # compute final accuracy on training and test sets\n",
        "    y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
        "    tr_acc = accuracy(tr_y, y_pred)\n",
        "    y_pred = model.predict([dev_pairs[:, 0], dev_pairs[:, 1]])\n",
        "    dev_acc = accuracy(dev_y, y_pred)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "    print('* Accuracy on dev set: %0.2f%%' % (100 * dev_acc))\n",
        "    \n",
        "    return -dev_acc\n",
        "\n",
        "x0 = [128, 'rms', 512, 20, 0.00001, 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPptTE4gShhV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_pairs])\n",
        "print('* Baseline accuracy on dev set: %0.2f%%' % (100 * accuracy(dev_y, baseline_similarities)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1i_j4YWy2La",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_filename = \"{}-checkpoint.pkl\".format(time.asctime())\n",
        "checkpoint_filepath = os.path.join(os.curdir, checkpoint_filename)\n",
        "\n",
        "def backup(res):\n",
        "    skopt.dump(res, checkpoint_filepath)\n",
        "    uploaded = drive.CreateFile({'title': checkpoint_filename})\n",
        "    uploaded.SetContentFile(checkpoint_filepath)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "res = skopt.gp_minimize(objective, space, x0=x0, n_calls=25, callback=[backup])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoUX1lm4MxMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a26a51a2-df9e-49c3-a00d-2a4ee01e00b8"
      },
      "cell_type": "code",
      "source": [
        "res.x"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[254, 'rms', 256, 20, 0.0010632850269373967, 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "z-wcc2Yfolmo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}