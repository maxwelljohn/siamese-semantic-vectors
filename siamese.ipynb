{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e58dd94b-4429-46f8-f5eb-842355edc03c"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "import sys\n",
        "import time\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.constraints import non_neg\n",
        "from scipy.misc import imread\n",
        "\n",
        "!pip install scikit-optimize\n",
        "import skopt\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    a, b = np.ravel(a), np.ravel(b)\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / max(np.linalg.norm(a) * np.linalg.norm(b), sys.float_info.epsilon)\n",
        "\n",
        "\n",
        "def keras_norm(vect):\n",
        "    return K.sqrt(K.batch_dot(vect, vect, axes=1))\n",
        "\n",
        "\n",
        "def keras_similarity(vects):\n",
        "    x, y = vects\n",
        "    # Cosine similarity\n",
        "    return K.batch_dot(x, y, axes=1) / K.maximum(keras_norm(x) * keras_norm(y), K.epsilon())\n",
        "\n",
        "\n",
        "def sim_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, output_size=128, map_reg_rate=0, scale_reg_rate=0):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = input\n",
        "    x = Dense(output_size, kernel_regularizer=regularizers.l2(map_reg_rate))(x)\n",
        "    x = Dense(output_size, kernel_initializer='identity', kernel_constraint=non_neg(),\n",
        "              kernel_regularizer=regularizers.l2(scale_reg_rate), use_bias=False)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = y_pred.ravel() > median\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def keras_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.75, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxHK8Dd1z4k1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ce25bdd9-b575-4d92-ba19-9d15fb007e8b"
      },
      "cell_type": "code",
      "source": [
        "!unzip -u glove.6B.zip || (wget http://nlp.stanford.edu/data/glove.6B.zip && unzip -u glove.6B.zip)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\r\n",
            "--2018-08-30 14:42:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2018-08-30 14:42:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  16.7MB/s    in 39s     \n",
            "\n",
            "2018-08-30 14:43:06 (20.9 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFVWMt_f0fWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 50K words is enough for 95% of English word usage per the OED:\n",
        "# https://web.archive.org/web/20160304170936/http://www.oxforddictionaries.com/words/the-oec-facts-about-the-language\n",
        "# The GloVe text files appear to be roughly sorted by frequency of usage.\n",
        "!egrep '^[a-z]+ ' glove.6B.300d.txt > glove.head.txt\n",
        "!cut -d' ' -f1 glove.head.txt > glove.head.strings.txt\n",
        "!cut -d' ' -f2- glove.head.txt > glove.head.vectors.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2Q9l3PN110t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_strings = np.loadtxt('glove.head.strings.txt', dtype=object)\n",
        "word_vectors = np.loadtxt('glove.head.vectors.txt')\n",
        "input_shape = word_vectors.shape[1:]\n",
        "assert len(word_strings) == len(word_vectors)\n",
        "\n",
        "index_for_word = {}\n",
        "for i, word in enumerate(word_strings):\n",
        "    index_for_word[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T84gxLzJU9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "def are_synonyms(word1, word2):\n",
        "    lemma2 = wnl.lemmatize(word2)\n",
        "    for synset in wn.synsets(word1):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        if word2 in lemma_names or lemma2 in lemma_names:\n",
        "            return True\n",
        "    return False\n",
        "assert are_synonyms('car', 'auto')\n",
        "assert are_synonyms('auto', 'car')\n",
        "assert are_synonyms('car', 'railcar')\n",
        "assert not are_synonyms('car', 'airplane')\n",
        "\n",
        "\n",
        "def create_pairs(word_strings, word_vectors, class_indices, choose_hard_negs=1, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    string_pairs = []\n",
        "    vector_pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(word_strings))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "        num_outside_family = len(shuffled_indices) - len(family)\n",
        "        assert choose_hard_negs < num_outside_family\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            np.random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            string_pairs.append([word_strings[anchor], word_strings[pos]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[pos]])\n",
        "            labels.append(1)\n",
        "\n",
        "            hardest_neg = None\n",
        "            closest_similarity = float('-infinity')\n",
        "            candidates = 0\n",
        "            while candidates < choose_hard_negs:\n",
        "                try:\n",
        "                    random_neg = shuffled_indices[next_index]\n",
        "                    next_index += 1\n",
        "                    while random_neg in family or are_synonyms(word_strings[anchor], word_strings[random_neg]):\n",
        "                        random_neg = shuffled_indices[next_index]\n",
        "                        next_index += 1\n",
        "                except IndexError:\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                    continue\n",
        "                sim = similarity(word_vectors[anchor], word_vectors[random_neg])\n",
        "                if sim > closest_similarity:\n",
        "                    hardest_neg = random_neg\n",
        "                    closest_similarity = sim\n",
        "                candidates += 1\n",
        "            string_pairs.append([word_strings[anchor], word_strings[hardest_neg]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[hardest_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(string_pairs) == len(vector_pairs) == len(labels)\n",
        "    return np.array(string_pairs), np.array(vector_pairs), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6A3h9Wj2Xs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "syn_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_num = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        syn_indices = [index_for_word[syn] for syn in synsets[0].lemma_names() if syn in index_for_word and syn_class[index_for_word[syn]] == -1]\n",
        "        if len(syn_indices) > 1:\n",
        "            syn_class[syn_indices] = class_num\n",
        "            class_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnnybeg0DajU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = list(range(class_num))\n",
        "random.shuffle(classes)\n",
        "train_dev_split = round(len(classes)*0.6)\n",
        "dev_test_split = round(len(classes)*0.8)\n",
        "train_classes = classes[:train_dev_split]\n",
        "dev_classes = classes[train_dev_split:dev_test_split]\n",
        "test_classes = classes[dev_test_split:]\n",
        "assert len(set(train_classes).intersection(set(dev_classes))) == 0\n",
        "assert len(set(dev_classes).intersection(set(test_classes))) == 0\n",
        "assert len(set(train_classes).intersection(set(test_classes))) == 0\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in train_classes]\n",
        "tr_strings, tr_pairs, tr_y = create_pairs(word_strings, word_vectors, class_indices)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in dev_classes]\n",
        "dev_strings, dev_pairs, dev_y = create_pairs(word_strings, word_vectors, class_indices)\n",
        "\n",
        "class_indices = [np.where(syn_class == c)[0] for c in test_classes]\n",
        "te_strings, te_pairs, te_y = create_pairs(word_strings, word_vectors, class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sDnYWX3IT8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Integer(1, 300, name='output_size'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([30], name='epochs'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='map_reg_rate'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='scale_reg_rate'),\n",
        "    skopt.space.Integer(1, 50, name='deviation_dropoff'),\n",
        "]\n",
        "\n",
        "@skopt.utils.use_named_args(space)\n",
        "def objective(output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff):\n",
        "    print(locals())\n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, output_size)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(keras_similarity,\n",
        "                      output_shape=sim_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "\n",
        "    def contrastive_sim_loss(y_true, y_pred):\n",
        "        return K.mean(y_true * (-y_pred + 1) +\n",
        "                      (1 - y_true) * (K.maximum(y_pred, 0) ** deviation_dropoff))\n",
        "    \n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_sim_loss, optimizer=opt, metrics=[keras_accuracy])\n",
        "    model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=0,\n",
        "        validation_data=([dev_pairs[:, 0], dev_pairs[:, 1]], dev_y))\n",
        "    \n",
        "    # compute final accuracy on training and test sets\n",
        "    y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
        "    tr_acc = accuracy(tr_y, y_pred)\n",
        "    y_pred = model.predict([dev_pairs[:, 0], dev_pairs[:, 1]])\n",
        "    dev_acc = accuracy(dev_y, y_pred)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "    print('* Accuracy on dev set: %0.2f%%' % (100 * dev_acc))\n",
        "    \n",
        "    return -dev_acc\n",
        "\n",
        "x0 = [270, 'rms', 256, 30, 0.01, 0.01, 14]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPptTE4gShhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a608f256-7620-46e9-aa72-e5e9a1ac3001"
      },
      "cell_type": "code",
      "source": [
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_pairs])\n",
        "print('* Baseline accuracy on dev set: %0.2f%%' % (100 * accuracy(dev_y, baseline_similarities)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Baseline accuracy on dev set: 75.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p1i_j4YWy2La",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2162
        },
        "outputId": "2baba36d-0dd8-48a4-d8b3-82fe1e3bde5e"
      },
      "cell_type": "code",
      "source": [
        "checkpoint_filename = \"{}-checkpoint.pkl\".format(time.asctime())\n",
        "checkpoint_filepath = os.path.join(os.curdir, checkpoint_filename)\n",
        "\n",
        "def backup(res):\n",
        "    skopt.dump(res, checkpoint_filepath)\n",
        "    uploaded = drive.CreateFile({'title': checkpoint_filename})\n",
        "    uploaded.SetContentFile(checkpoint_filepath)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "res = skopt.gp_minimize(objective, space, x0=x0, n_calls=25, callback=[backup])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'scale_reg_rate': 0.01, 'map_reg_rate': 0.01, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 270, 'deviation_dropoff': 14}\n",
            "* Optimization took 53 seconds\n",
            "* Accuracy on training set: 91.47%\n",
            "* Accuracy on dev set: 84.78%\n",
            "Uploaded file with ID 1xf1SMuUyM0GAX2w-BziKqr2Ik6TXXqug\n",
            "{'scale_reg_rate': 4.502841285115963e-07, 'map_reg_rate': 0.03380882973914102, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 212, 'deviation_dropoff': 2}\n",
            "* Optimization took 95 seconds\n",
            "* Accuracy on training set: 88.35%\n",
            "* Accuracy on dev set: 83.87%\n",
            "Uploaded file with ID 192Cb20WMVSk7YOGFSPPlJB4qj_Pa1sa5\n",
            "{'scale_reg_rate': 3.24861652197352e-05, 'map_reg_rate': 0.1868667624047457, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 35, 'deviation_dropoff': 22}\n",
            "* Optimization took 95 seconds\n",
            "* Accuracy on training set: 91.79%\n",
            "* Accuracy on dev set: 84.88%\n",
            "Uploaded file with ID 1pCU9MvVzhK5Y-1SwHJWzCMhju0Wle1yb\n",
            "{'scale_reg_rate': 3.6233263996165756e-07, 'map_reg_rate': 7.446293891654321e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 100, 'deviation_dropoff': 4}\n",
            "* Optimization took 38 seconds\n",
            "* Accuracy on training set: 89.28%\n",
            "* Accuracy on dev set: 84.73%\n",
            "Uploaded file with ID 106fHWVUWX2ydsJfNPG09iZOk6JxIe4zM\n",
            "{'scale_reg_rate': 4.012463538991295e-07, 'map_reg_rate': 8.102752672370561e-07, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 244, 'deviation_dropoff': 38}\n",
            "* Optimization took 98 seconds\n",
            "* Accuracy on training set: 92.19%\n",
            "* Accuracy on dev set: 84.64%\n",
            "Uploaded file with ID 1YY_hIIbAqfxwzdjgZF3A4T-ahwwc8gP2\n",
            "{'scale_reg_rate': 0.0007051451117779907, 'map_reg_rate': 0.37866687767399165, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 14, 'deviation_dropoff': 6}\n",
            "* Optimization took 37 seconds\n",
            "* Accuracy on training set: 89.50%\n",
            "* Accuracy on dev set: 84.40%\n",
            "Uploaded file with ID 1of_XrtPH48DcIT7WP94h2pFLOxspk7VE\n",
            "{'scale_reg_rate': 5.4799325637651, 'map_reg_rate': 70.89712535119247, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 166, 'deviation_dropoff': 7}\n",
            "* Optimization took 66 seconds\n",
            "* Accuracy on training set: 90.61%\n",
            "* Accuracy on dev set: 84.85%\n",
            "Uploaded file with ID 12P2thTlVdJKtsVaoQrYdY1VQuC2jglkN\n",
            "{'scale_reg_rate': 0.03094901291483512, 'map_reg_rate': 73.68761603581014, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 216, 'deviation_dropoff': 50}\n",
            "* Optimization took 25 seconds\n",
            "* Accuracy on training set: 92.86%\n",
            "* Accuracy on dev set: 84.66%\n",
            "Uploaded file with ID 1QmOcO93u_RD8hYUGFOAYKdfpMvMIrS-H\n",
            "{'scale_reg_rate': 7.57646851796507e-08, 'map_reg_rate': 18.47777782052265, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 253, 'deviation_dropoff': 34}\n",
            "* Optimization took 32 seconds\n",
            "* Accuracy on training set: 92.38%\n",
            "* Accuracy on dev set: 84.85%\n",
            "Uploaded file with ID 1XdcUEa2hMNMt-a-17fxnKJB0-PDe_3pM\n",
            "{'scale_reg_rate': 3.5477685489508776e-05, 'map_reg_rate': 78.09211659119056, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 60, 'deviation_dropoff': 9}\n",
            "* Optimization took 91 seconds\n",
            "* Accuracy on training set: 90.95%\n",
            "* Accuracy on dev set: 84.88%\n",
            "Uploaded file with ID 1dVYUCTi4lmQZQT-TKo-vX5uZbPmpv1k6\n",
            "{'scale_reg_rate': 0.0001270898610099444, 'map_reg_rate': 0.037014636147642666, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 123, 'deviation_dropoff': 19}\n",
            "* Optimization took 99 seconds\n",
            "* Accuracy on training set: 91.75%\n",
            "* Accuracy on dev set: 85.02%\n",
            "Uploaded file with ID 1Q-r42Q4-k8Ey53qCUFYYgyJD5n9hx-bv\n",
            "{'scale_reg_rate': 622.699938427126, 'map_reg_rate': 2.8265666845889525e-08, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 1, 'deviation_dropoff': 48}\n",
            "* Optimization took 30 seconds\n",
            "* Accuracy on training set: 50.00%\n",
            "* Accuracy on dev set: 50.00%\n",
            "Uploaded file with ID 1w4X0FzltpEN7GN6OmMFJ3EC-LGsQAF81\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 230, 'deviation_dropoff': 50}\n",
            "* Optimization took 19 seconds\n",
            "* Accuracy on training set: 92.68%\n",
            "* Accuracy on dev set: 84.71%\n",
            "Uploaded file with ID 1JsIKF70UCmUcJyCwJethPP-Sp6l27HiB\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 1}\n",
            "* Optimization took 33 seconds\n",
            "* Accuracy on training set: 86.44%\n",
            "* Accuracy on dev set: 82.48%\n",
            "Uploaded file with ID 1U_REn_rT1STTMP3RSLLKcxTJUotkREOS\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 15}\n",
            "* Optimization took 38 seconds\n",
            "* Accuracy on training set: 91.33%\n",
            "* Accuracy on dev set: 84.95%\n",
            "Uploaded file with ID 1ffGJMF51s9B97DztWQSdhFAsypSj6RWP\n",
            "{'scale_reg_rate': 1000.0, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 27}\n",
            "* Optimization took 19 seconds\n",
            "* Accuracy on training set: 92.14%\n",
            "* Accuracy on dev set: 84.61%\n",
            "Uploaded file with ID 1M_poUuj1A4Hi3T4YR_k03X5iXdca3kou\n",
            "{'scale_reg_rate': 1e-09, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 1, 'deviation_dropoff': 31}\n",
            "* Optimization took 24 seconds\n",
            "* Accuracy on training set: 50.00%\n",
            "* Accuracy on dev set: 50.00%\n",
            "Uploaded file with ID 1qNfRS23D5UFL7yhWVnHhzMofr-dWl-W_\n",
            "{'scale_reg_rate': 1.9304057420470522e-05, 'map_reg_rate': 1.9006975657616626e-09, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 157, 'deviation_dropoff': 50}\n",
            "* Optimization took 44 seconds\n",
            "* Accuracy on training set: 92.90%\n",
            "* Accuracy on dev set: 84.37%\n",
            "Uploaded file with ID 1ZcZE4NEGic0Ha-Iv7JYBuZOyE2q9eIP4\n",
            "{'scale_reg_rate': 306.4050397008083, 'map_reg_rate': 12.227143359474894, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 1, 'deviation_dropoff': 50}\n",
            "* Optimization took 82 seconds\n",
            "* Accuracy on training set: 50.00%\n",
            "* Accuracy on dev set: 50.00%\n",
            "Uploaded file with ID 1cORHOCyI3lUGUL-nlYxlbIV0YC1T1_yQ\n",
            "{'scale_reg_rate': 4.3051697995210425, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 1}\n",
            "* Optimization took 47 seconds\n",
            "* Accuracy on training set: 86.39%\n",
            "* Accuracy on dev set: 82.05%\n",
            "Uploaded file with ID 16k3EUAkaXAHALNyFQWdgjJ9UP56qsSG-\n",
            "{'scale_reg_rate': 2.093848234162921e-06, 'map_reg_rate': 8.71279164199795, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 45, 'deviation_dropoff': 20}\n",
            "* Optimization took 77 seconds\n",
            "* Accuracy on training set: 92.15%\n",
            "* Accuracy on dev set: 84.71%\n",
            "Uploaded file with ID 10B0h5yqqX8GKr7-_CMsFb1DETBjD_JPU\n",
            "{'scale_reg_rate': 5.740886930893359e-05, 'map_reg_rate': 1000.0, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 300, 'deviation_dropoff': 50}\n",
            "* Optimization took 19 seconds\n",
            "* Accuracy on training set: 92.61%\n",
            "* Accuracy on dev set: 84.66%\n",
            "Uploaded file with ID 1LKJQc4pJ-pw0rAeqnyY-FpZwq8RHjgOj\n",
            "{'scale_reg_rate': 1000.0, 'map_reg_rate': 1e-09, 'epochs': 30, 'batch_size': 256, 'optimizer': 'rms', 'output_size': 231, 'deviation_dropoff': 1}\n",
            "* Optimization took 45 seconds\n",
            "* Accuracy on training set: 86.52%\n",
            "* Accuracy on dev set: 82.33%\n",
            "Uploaded file with ID 16T5d9u0rdlOks7Rj-eLrDcTEgonpwMPC\n",
            "{'scale_reg_rate': 0.27233877325014416, 'map_reg_rate': 0.2705063936301946, 'epochs': 30, 'batch_size': 512, 'optimizer': 'rms', 'output_size': 20, 'deviation_dropoff': 1}\n",
            "* Optimization took 25 seconds\n",
            "* Accuracy on training set: 86.00%\n",
            "* Accuracy on dev set: 82.48%\n",
            "Uploaded file with ID 13NZsognK2hvCLD70Gc7BwnwyBEtU__ZP\n",
            "{'scale_reg_rate': 9.02133505260208e-09, 'map_reg_rate': 2.141271690133445e-09, 'epochs': 30, 'batch_size': 128, 'optimizer': 'rms', 'output_size': 185, 'deviation_dropoff': 28}\n",
            "* Optimization took 77 seconds\n",
            "* Accuracy on training set: 92.25%\n",
            "* Accuracy on dev set: 84.66%\n",
            "Uploaded file with ID 1uHazgB-ht_QsaNFkIlHQylcIXVfFlRvJ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qoUX1lm4MxMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ad29c7b-716e-4ab9-f488-beea49730c5c"
      },
      "cell_type": "code",
      "source": [
        "res.x"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123, 'rms', 128, 30, 0.037014636147642666, 0.0001270898610099444, 19]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "z-wcc2Yfolmo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}