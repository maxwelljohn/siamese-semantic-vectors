{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxwelljohn/siamese-word2vec/blob/master/siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fv-b7O6mz7G6",
        "colab_type": "code",
        "outputId": "c724d569-f790-4272-b71a-c8cb9611390f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import skimage.transform\n",
        "import sys\n",
        "import time\n",
        "import unittest\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.constraints import non_neg\n",
        "from scipy.misc import imread\n",
        "\n",
        "!pip install scikit-optimize\n",
        "import skopt\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.20.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.6)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85s8R9-WEyOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    a, b = np.ravel(a), np.ravel(b)\n",
        "    # Cosine similarity\n",
        "    return np.dot(a, b) / max(np.linalg.norm(a) * np.linalg.norm(b), sys.float_info.epsilon)\n",
        "\n",
        "\n",
        "def keras_norm(vect):\n",
        "    return K.sqrt(K.batch_dot(vect, vect, axes=1))\n",
        "\n",
        "\n",
        "def keras_similarity(vects):\n",
        "    x, y = vects\n",
        "    # Cosine similarity\n",
        "    return K.batch_dot(x, y, axes=1) / K.maximum(keras_norm(x) * keras_norm(y), K.epsilon())\n",
        "\n",
        "\n",
        "def sim_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def create_base_network(input_shape, output_size=128, map_reg_rate=0, scale_reg_rate=0):\n",
        "    '''Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    input = Input(shape=input_shape)\n",
        "    x = input\n",
        "    x = Dense(output_size, kernel_regularizer=regularizers.l2(map_reg_rate))(x)\n",
        "    x = Dense(output_size, kernel_initializer='identity', kernel_constraint=non_neg(),\n",
        "              kernel_regularizer=regularizers.l2(scale_reg_rate), use_bias=False)(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "\n",
        "def syn_accuracy(y_true, y_pred):\n",
        "    '''Compute synonym classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = y_pred.ravel() > median\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def ant_accuracy(y_true, y_pred):\n",
        "    '''Compute antonym classification accuracy with a variable threshold on similarities.\n",
        "    '''\n",
        "    median = np.median(y_pred)\n",
        "    pred = -((y_pred.ravel() < median).astype(np.int, casting='safe', copy=False))\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "\n",
        "def keras_syn_accuracy(y_true, y_pred):\n",
        "    '''Compute synonym classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.75, y_true.dtype)))\n",
        "\n",
        "\n",
        "def keras_ant_accuracy(y_true, y_pred):\n",
        "    '''Compute antonym classification accuracy with a fixed threshold on similarities.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < -0.75, y_true.dtype)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxHK8Dd1z4k1",
        "colab_type": "code",
        "outputId": "0af1db20-1bfb-4900-b5e7-f562614f9096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip -u glove.6B.zip || (wget http://nlp.stanford.edu/data/glove.6B.zip && unzip -u glove.6B.zip)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n",
            "--2019-01-22 23:44:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-01-22 23:44:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  22.8MB/s    in 28s     \n",
            "\n",
            "2019-01-22 23:45:26 (29.0 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFVWMt_f0fWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 50K words is enough for 95% of English word usage per the OED:\n",
        "# https://web.archive.org/web/20160304170936/http://www.oxforddictionaries.com/words/the-oec-facts-about-the-language\n",
        "# The GloVe text files appear to be roughly sorted by frequency of usage.\n",
        "!egrep '^[a-z]+ ' glove.6B.300d.txt > glove.head.txt\n",
        "!cut -d' ' -f1 glove.head.txt > glove.head.strings.txt\n",
        "!cut -d' ' -f2- glove.head.txt > glove.head.vectors.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2Q9l3PN110t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_strings = np.loadtxt('glove.head.strings.txt', dtype=object)\n",
        "word_vectors = np.loadtxt('glove.head.vectors.txt')\n",
        "input_shape = word_vectors.shape[1:]\n",
        "assert len(word_strings) == len(word_vectors)\n",
        "\n",
        "index_for_word = {}\n",
        "for i, word in enumerate(word_strings):\n",
        "    index_for_word[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T84gxLzJU9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2e61f49e-5ee2-4db0-bf6c-f9de2f83a6c3"
      },
      "cell_type": "code",
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def might_be_synonyms(w1, w2):\n",
        "    s1 = set()\n",
        "    d1 = set()\n",
        "    s2 = set()\n",
        "    d2 = set()\n",
        "    for synset in wn.synsets(w1):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        s1.update(lemma_names)\n",
        "        d1.update(nltk.word_tokenize(synset.definition()))\n",
        "    for synset in wn.synsets(w2):\n",
        "        lemma_names = synset.lemma_names()\n",
        "        s2.update(lemma_names)\n",
        "        d2.update(nltk.word_tokenize(synset.definition()))\n",
        "    total_intersection = len(s1.intersection(s2)) + len(d1.intersection(s2)) + len(d2.intersection(s1))\n",
        "    return total_intersection > 0\n",
        "\n",
        "\n",
        "class TestSynonyms(unittest.TestCase):\n",
        "    def test_true(self):\n",
        "        pairs = [\n",
        "            ('car', 'auto'),\n",
        "            ('auto', 'car'),\n",
        "            ('car', 'railcar'),\n",
        "            ('small', 'tiny'),\n",
        "            ('small', 'miniature'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertTrue(might_be_synonyms(word1, word2))\n",
        "    def test_false(self):\n",
        "        pairs = [\n",
        "            ('car', 'airplane'),\n",
        "            ('car', 'fast'),\n",
        "            ('small', 'focused'),\n",
        "            ('small', 'accidental'),\n",
        "            ('small', 'rabbit'),\n",
        "            ('small', 'flippant'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertFalse(might_be_synonyms(word1, word2))\n",
        "\n",
        "\n",
        "def get_antonyms(word):\n",
        "    result = set()\n",
        "    for lemma in wn.lemmas(word):\n",
        "        for antonym in lemma.antonyms():\n",
        "            result.add(antonym.name())\n",
        "    return result\n",
        "\n",
        "\n",
        "def might_be_antonyms(w1, w2):\n",
        "    w1_antonyms = get_antonyms(w1)\n",
        "    w2_antonyms = get_antonyms(w2)\n",
        "    for w1_antonym in w1_antonyms:\n",
        "        if might_be_synonyms(w2, w1_antonym):\n",
        "            return True\n",
        "    for w2_antonym in w2_antonyms:\n",
        "        if might_be_synonyms(w1, w2_antonym):\n",
        "            return True\n",
        "\n",
        "        \n",
        "class TestAntonyms(unittest.TestCase):\n",
        "    def test_get_antonyms(self):\n",
        "        self.assertEqual(get_antonyms('big'), set(['little', 'small']))\n",
        "        self.assertEqual(get_antonyms('fast'), set(['slow']))\n",
        "        self.assertEqual(get_antonyms('big'), set(['little', 'small']))\n",
        "    def test_true(self):\n",
        "        pairs = [\n",
        "            ('big', 'small'),\n",
        "            ('big', 'minor'),\n",
        "            ('big', 'tiny'),\n",
        "            ('big', 'miniature'),\n",
        "            ('fast', 'slow'),\n",
        "            ('fast', 'sluggish'),\n",
        "            ('loud', 'soft'),\n",
        "            ('loud', 'quiet'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertTrue(might_be_antonyms(word1, word2))\n",
        "    def test_false(self):\n",
        "        pairs = [\n",
        "            ('big', 'huge'),\n",
        "            ('big', 'clean'),\n",
        "            ('fast', 'speedy'),\n",
        "            ('fast', 'unusual'),\n",
        "            ('loud', 'noisy'),\n",
        "            ('loud', 'flat'),\n",
        "        ]\n",
        "        for word1, word2 in pairs:\n",
        "            self.assertFalse(might_be_antonyms(word1, word2))\n",
        "\n",
        "            \n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
        "\n",
        "\n",
        "def create_pairs(word_strings, word_vectors, class_indices, pos_label, max_per_class=float('infinity')):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    string_pairs = []\n",
        "    vector_pairs = []\n",
        "    labels = []\n",
        "    for family in class_indices:\n",
        "        sibling_pairs = np.array(list(itertools.combinations(family, 2)))\n",
        "\n",
        "        shuffled_indices = np.arange(len(word_strings))\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        next_index = 0\n",
        "\n",
        "        if len(sibling_pairs) > max_per_class:\n",
        "            sibling_pairs = sibling_pairs[np.random.choice(len(sibling_pairs), max_per_class)]\n",
        "\n",
        "        for sibling_pair in sibling_pairs:\n",
        "            np.random.shuffle(sibling_pair)\n",
        "            anchor, pos = sibling_pair\n",
        "            string_pairs.append([word_strings[anchor], word_strings[pos]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[pos]])\n",
        "            labels.append(pos_label)\n",
        "            \n",
        "            random_neg = None\n",
        "            while random_neg == None or random_neg in family or \\\n",
        "                    might_be_synonyms(word_strings[anchor], word_strings[random_neg]) or \\\n",
        "                    might_be_antonyms(word_strings[anchor], word_strings[random_neg]):\n",
        "                if next_index >= len(shuffled_indices):\n",
        "                    np.random.shuffle(shuffled_indices)\n",
        "                    next_index = 0\n",
        "                random_neg = shuffled_indices[next_index]\n",
        "                next_index += 1\n",
        "\n",
        "            string_pairs.append([word_strings[anchor], word_strings[random_neg]])\n",
        "            vector_pairs.append([word_vectors[anchor], word_vectors[random_neg]])\n",
        "            labels.append(0)\n",
        "    assert len(string_pairs) == len(vector_pairs) == len(labels)\n",
        "    return np.array(string_pairs), np.array(vector_pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def create_train_dev_test(word_strings, word_vectors, class_count, word_class, pos_label):\n",
        "    classes = list(range(class_count))\n",
        "    random.seed(850101)\n",
        "    random.shuffle(classes)\n",
        "    random.seed()\n",
        "    train_dev_split = round(len(classes)*0.6)\n",
        "    dev_test_split = round(len(classes)*0.8)\n",
        "    train_classes = classes[:train_dev_split]\n",
        "    dev_classes = classes[train_dev_split:dev_test_split]\n",
        "    test_classes = classes[dev_test_split:]\n",
        "    assert len(set(train_classes).intersection(set(dev_classes))) == 0\n",
        "    assert len(set(dev_classes).intersection(set(test_classes))) == 0\n",
        "    assert len(set(train_classes).intersection(set(test_classes))) == 0\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in train_classes]\n",
        "    tr_strings, tr_pairs, tr_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in dev_classes]\n",
        "    dev_strings, dev_pairs, dev_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "\n",
        "    class_indices = [np.where(word_class == c)[0] for c in test_classes]\n",
        "    te_strings, te_pairs, te_y = create_pairs(word_strings, word_vectors, class_indices, pos_label)\n",
        "    \n",
        "    return tr_strings, tr_pairs, tr_y, dev_strings, dev_pairs, dev_y, te_strings, te_pairs, te_y"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.106s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "x6A3h9Wj2Xs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "syn_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_count = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        syn_indices = [index_for_word[syn] for syn in synsets[0].lemma_names() if syn in index_for_word and syn_class[index_for_word[syn]] == -1]\n",
        "        if len(syn_indices) > 1:\n",
        "            syn_class[syn_indices] = class_count\n",
        "            class_count += 1\n",
        "\n",
        "tr_syn_strings, tr_syn_pairs, tr_syn_y, \\\n",
        "dev_syn_strings, dev_syn_pairs, dev_syn_y, \\\n",
        "te_syn_strings, te_syn_pairs, te_syn_y = create_train_dev_test(word_strings, word_vectors, class_count, syn_class, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o13AAwvvaIlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ant_class = -np.ones(len(word_strings), dtype=np.int)\n",
        "shuffled_indices = np.arange(len(word_strings))\n",
        "np.random.shuffle(shuffled_indices)\n",
        "class_count = 0\n",
        "for i in shuffled_indices:\n",
        "    word = word_strings[i]\n",
        "    antonyms = get_antonyms(word)\n",
        "    if antonyms:\n",
        "        j = 0\n",
        "        while j < len(antonyms) and antonyms[j] not in index_for_word:\n",
        "            j += 1\n",
        "        if j < len(antonyms):\n",
        "            ant_indices = [i, index_for_word[antonyms[j]]]\n",
        "            if all(ant_class[ant_indices] == -1):\n",
        "                ant_class[ant_indices] = class_count\n",
        "                class_count += 1\n",
        "\n",
        "tr_ant_strings, tr_ant_pairs, tr_ant_y, \\\n",
        "dev_ant_strings, dev_ant_pairs, dev_ant_y, \\\n",
        "te_ant_strings, te_ant_pairs, te_ant_y = create_train_dev_test(word_strings, word_vectors, class_count, ant_class, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sDnYWX3IT8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = [\n",
        "    skopt.space.Integer(1, 300, name='output_size'),\n",
        "    skopt.space.Categorical(['rms'], name='optimizer'),\n",
        "    skopt.space.Categorical([128, 256, 512], name='batch_size'),\n",
        "    skopt.space.Categorical([30], name='epochs'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='map_reg_rate'),\n",
        "    skopt.space.Real(0.000000001, 1000, prior='log-uniform', name='scale_reg_rate'),\n",
        "    skopt.space.Integer(1, 50, name='deviation_dropoff'),\n",
        "]\n",
        "\n",
        "#@skopt.utils.use_named_args(space)\n",
        "def objective(tr_data, output_size, optimizer, batch_size, epochs, map_reg_rate, scale_reg_rate, deviation_dropoff):\n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    # network definition\n",
        "    base_network = create_base_network(input_shape, output_size)\n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(keras_similarity,\n",
        "                      output_shape=sim_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    model = Model([input_a, input_b], distance)\n",
        "\n",
        "    def contrastive_sim_loss(y_true, y_pred):\n",
        "        return K.mean(-y_true * y_pred +\n",
        "                      K.cast(K.equal(y_true, 0), 'float32') * (K.abs(y_pred) ** deviation_dropoff))\n",
        "\n",
        "    # train\n",
        "    if optimizer == 'rms':\n",
        "        opt = RMSprop()\n",
        "    else:\n",
        "        raise ValueError(\"unknown optimizer\")\n",
        "    model.compile(loss=contrastive_sim_loss, optimizer=opt)\n",
        "    \n",
        "    for tr_pairs, tr_y in tr_data:\n",
        "        model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=0)\n",
        "    \n",
        "    y_pred = model.predict([tr_syn_pairs[:, 0], tr_syn_pairs[:, 1]])\n",
        "    syn_tr_acc = syn_accuracy(tr_syn_y, y_pred)\n",
        "    y_pred = model.predict([dev_syn_pairs[:, 0], dev_syn_pairs[:, 1]])\n",
        "    syn_dev_acc = syn_accuracy(dev_syn_y, y_pred)\n",
        "\n",
        "    y_pred = model.predict([tr_ant_pairs[:, 0], tr_ant_pairs[:, 1]])\n",
        "    ant_tr_acc = ant_accuracy(tr_ant_y, y_pred)\n",
        "    y_pred = model.predict([dev_ant_pairs[:, 0], dev_ant_pairs[:, 1]])\n",
        "    ant_dev_acc = ant_accuracy(dev_ant_y, y_pred)\n",
        "\n",
        "    print('* Optimization took {:.0f} seconds'.format((datetime.datetime.now() - start_time).total_seconds()))\n",
        "    print('* Accuracy on synonym training set: %0.2f%%' % (100 * syn_tr_acc))\n",
        "    print('* Accuracy on synonym dev set: %0.2f%%' % (100 * syn_dev_acc))\n",
        "    print('* Accuracy on antonym training set: %0.2f%%' % (100 * ant_tr_acc))\n",
        "    print('* Accuracy on antonym dev set: %0.2f%%' % (100 * ant_dev_acc))\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPptTE4gShhV",
        "colab_type": "code",
        "outputId": "2a800ce3-16d0-4d0b-8697-1b0528bfcd85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_syn_pairs])\n",
        "print('* Baseline synonym accuracy on dev set: %0.2f%%' % (100 * syn_accuracy(dev_syn_y, baseline_similarities)))\n",
        "baseline_similarities = np.array([similarity(p[0], p[1]) for p in dev_ant_pairs])\n",
        "print('* Baseline antonym accuracy on dev set: %0.2f%%' % (100 * ant_accuracy(dev_ant_y, baseline_similarities)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Baseline synonym accuracy on dev set: 76.59%\n",
            "* Baseline antonym accuracy on dev set: 11.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l562zedZzM_N",
        "colab_type": "code",
        "outputId": "4ad0237b-6b69-4581-801a-079fdd149c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "print('Training on synonyms...')\n",
        "objective([(tr_syn_pairs, tr_syn_y)], 250, 'rms', 256, 30, 0.01, 0.00001, 20)\n",
        "print('Training on antonyms...')\n",
        "objective([(tr_ant_pairs, tr_ant_y)], 250, 'rms', 256, 30, 0.01, 0.00001, 20)\n",
        "print('Training on synonyms & antonyms...')\n",
        "objective([(np.vstack((tr_syn_pairs, tr_ant_pairs)), np.hstack((tr_syn_y, tr_ant_y)))], 250, 'rms', 256, 30, 0.01, 0.00001, 20)\n",
        "print('Training on synonyms, then antonyms...')\n",
        "objective([(tr_syn_pairs, tr_syn_y), (tr_ant_pairs, tr_ant_y)], 250, 'rms', 256, 30, 0.01, 0.00001, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on synonyms...\n",
            "* Optimization took 21 seconds\n",
            "* Accuracy on synonym training set: 91.90%\n",
            "* Accuracy on synonym dev set: 84.75%\n",
            "* Accuracy on antonym training set: 11.15%\n",
            "* Accuracy on antonym dev set: 10.40%\n",
            "\n",
            "Training on antonyms...\n",
            "* Optimization took 5 seconds\n",
            "* Accuracy on synonym training set: 57.57%\n",
            "* Accuracy on synonym dev set: 58.87%\n",
            "* Accuracy on antonym training set: 72.91%\n",
            "* Accuracy on antonym dev set: 57.92%\n",
            "\n",
            "Training on synonyms & antonyms...\n",
            "* Optimization took 23 seconds\n",
            "* Accuracy on synonym training set: 89.07%\n",
            "* Accuracy on synonym dev set: 81.73%\n",
            "* Accuracy on antonym training set: 30.14%\n",
            "* Accuracy on antonym dev set: 27.72%\n",
            "\n",
            "Training on synonyms, then antonyms...\n",
            "* Optimization took 24 seconds\n",
            "* Accuracy on synonym training set: 57.96%\n",
            "* Accuracy on synonym dev set: 59.61%\n",
            "* Accuracy on antonym training set: 72.75%\n",
            "* Accuracy on antonym dev set: 58.91%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p1i_j4YWy2La",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_filename = \"{}-checkpoint.pkl\".format(time.asctime())\n",
        "checkpoint_filepath = os.path.join(os.curdir, checkpoint_filename)\n",
        "\n",
        "def backup(res):\n",
        "    skopt.dump(res, checkpoint_filepath)\n",
        "    uploaded = drive.CreateFile({'title': checkpoint_filename})\n",
        "    uploaded.SetContentFile(checkpoint_filepath)\n",
        "    uploaded.Upload()\n",
        "    print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "#res = skopt.gp_minimize(objective, space, x0=x0, n_calls=25, callback=[backup])\n",
        "#res.x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}